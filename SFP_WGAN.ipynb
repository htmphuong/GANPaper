{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bc954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "hybrid_vanilla_gan_1_pc2.csv 1 100\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/pc2hybrid_vanilla_gan_1_pc2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17220/2397767303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/pc2hybrid_vanilla_gan_1_pc2.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from dataloader import load_data\n",
    "from helpers import get_cat_dims\n",
    "import pandas as pd\n",
    "from models import WGANGP\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "results = {}\n",
    "epochs = [100]\n",
    "imputer = SimpleImputer()\n",
    "def fill_na(data):\n",
    "  # Fill NA value with mean\n",
    "  data = data.fillna(data.mean())\n",
    "  return data\n",
    "def preprocess_trainingData(X, y):\n",
    "    return reduce_size(normalize(handle_missing(X))[0])[0], y\n",
    "def reduce_size(X):\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca.fit(X)\n",
    "    return pca.transform(X), pca\n",
    "\n",
    "def transform_label_to_numeric(y, y_true=[b'true','true', 'TRUE', 'yes', 'Y', 'y',b'Y',\"b'Y'\",b'1',\"b'1'\",1,1.0,str(1), \"RA\",\"SN\",\"DN\",\"PL\"], y_false=[b'false','false', 'FALSE', 'no', 'N', 'n',b'N',\"b'N'\",b'0',\"b'0'\",0,0.0,str(0)]):\n",
    "    temp = y.copy()\n",
    "    #temp[np.isin(temp, y_true)] = 1\n",
    "    #temp[np.isin(temp, y_false)] = 0\n",
    "\n",
    "    #return np.asarray(temp, dtype=np.int)\n",
    "    return np.isin(temp, y_true).astype(np.int)\n",
    "def normalize(X):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    scaled = scaler.transform(X)\n",
    "    return scaled, scaler\n",
    "def handle_missing(data):\n",
    "    #data = data.fillna(data.mean())\n",
    "    #return data\n",
    "    \n",
    "    return imputer.fit_transform(data)\n",
    "\n",
    "def append_record(record):\n",
    "    with open('./results/individual_results_wgangp.txt', 'w') as f:\n",
    "        f.write(str(record))\n",
    "        f.write(os.linesep)\n",
    "\n",
    "path = \"./data/pc2\"\n",
    "for epoch in epochs :\n",
    "      for name in os.listdir(path) :\n",
    "        if name.endswith(\".csv\") :\n",
    "          for i in range(1) :\n",
    "            print(\"start\")\n",
    "            print()\n",
    "            print(name, i+1, epoch)\n",
    "            print()\n",
    "            print()\n",
    "            df = pd.read_csv(path+name, header=None)\n",
    "            \n",
    "            df.loc[df[df.columns[-1]]>=1, df.columns[-1]] = 1\n",
    "            print(df.shape)\n",
    "            cat_cols = None\n",
    "            num_cols = list(df.columns[:-1])\n",
    "            target_col = df.columns[-1]\n",
    "            if cat_cols is not None :\n",
    "                X = df.loc[:, num_cols + cat_cols]\n",
    "            else :\n",
    "                X = df.loc[:, num_cols]\n",
    "            y = df.loc[:, target_col]\n",
    "            if i == 0:\n",
    "              results[name] = {}\n",
    "            results[name][i+1] = {\"wgangp\" : {}}\n",
    "            preprocess_trainingData(X,y)\n",
    "            while(1) :\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=42*i, stratify=y)\n",
    "                if len(Counter(y_test)) > 1 :\n",
    "                    break\n",
    "\n",
    "            num_prep = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                                     MinMaxScaler())\n",
    "            cat_prep = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                                     OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            prep = ColumnTransformer([\n",
    "                ('num', num_prep, num_cols)],\n",
    "                remainder='drop')\n",
    "\n",
    "            cat_dims = get_cat_dims(X_train, cat_cols)\n",
    "\n",
    "            X_train_trans = prep.fit_transform(X_train)\n",
    "\n",
    "            gan = WGANGP(write_to_disk=True, # whether to create an output folder. Plotting will be surpressed if flase\n",
    "                        compute_metrics_every=1250, print_every=2500, plot_every=10000,\n",
    "                        num_cols = num_cols, cat_dims=cat_dims,\n",
    "                        # pass the one hot encoder to the GAN to enable count plots of categorical variables\n",
    "                        transformer=None,\n",
    "                        # pass column names to enable\n",
    "                        cat_cols=cat_cols,\n",
    "                        use_aux_classifier_loss=True,\n",
    "                        d_updates_per_g=3, gp_weight=15)\n",
    "\n",
    "            gan.fit(X_train_trans, y=y_train.values, \n",
    "                    condition=True,\n",
    "                    epochs=epoch,  \n",
    "                    batch_size=64,\n",
    "                    netG_kwargs = {'hidden_layer_sizes': (128,64), \n",
    "                                    'n_cross_layers': 1,\n",
    "                                    'cat_activation': 'gumbel_softmax',\n",
    "                                    'num_activation': 'none',\n",
    "                                    'condition_num_on_cat': False, \n",
    "                                    'noise_dim': 30, \n",
    "                                    'normal_noise': False,\n",
    "                                    'activation':  'leaky_relu',\n",
    "                                    'reduce_cat_dim': True,\n",
    "                                    'use_num_hidden_layer': True,\n",
    "                                    'layer_norm':False,},\n",
    "                    netD_kwargs = {'hidden_layer_sizes': (128,64,32),\n",
    "                                    'n_cross_layers': 2,\n",
    "                                    'embedding_dims': None,\n",
    "                                    'activation':  'leaky_relu',\n",
    "                                    'sigmoid_activation': False,\n",
    "                                    'noisy_num_cols': True,\n",
    "                                    'layer_norm':True,}\n",
    "                   )\n",
    "\n",
    "            X_res, y_res = gan.resample(X_train_trans, y=y_train)\n",
    "            y_res = np.resize(y_res, (y_res.shape[0], 1))\n",
    "            result = np.concatenate([X_res, y_res], axis=1)\n",
    "            df = pd.DataFrame(result)\n",
    "            print(result)\n",
    "            print(df.shape)\n",
    "            df.to_csv(path + \"hybrid_wgan_\"+str(i+1)+\"_\"+name, index=False, header=None)\n",
    "            \n",
    "            X_test_trans = prep.transform(X_test)\n",
    "            test_y = np.resize(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "            test = np.concatenate([X_test_trans, test_y], axis=1)\n",
    "            test = pd.DataFrame(test)\n",
    "            test.to_csv(path + \"test_\"+str(i+1)+\"_\"+name, header=None, index=False)\n",
    "\n",
    "            train_y = np.resize(y_train, (y_train.shape[0], 1))\n",
    "\n",
    "            train = np.concatenate([X_train_trans, train_y], axis=1)\n",
    "            train = pd.DataFrame(train)\n",
    "            train.to_csv(path + \"train_\"+str(i+1)+\"_\"+name, header=None, index=False)\n",
    "            clf = KNeighborsClassifier(5)\n",
    "            clf2 = KNeighborsClassifier(5)\n",
    "            clf.fit(X_res, y_res)\n",
    "            preds_oversampled = clf.predict_proba(X_test_trans)[:,1]\n",
    "\n",
    "            clf2.fit(X_train_trans, y_train)\n",
    "            preds_imbalanced = clf2.predict_proba(X_test_trans)[:,1]\n",
    "            hybrid1 = pd.read_csv(path + \"prop/hybrid_\"+str(i+1)+\"_\"+name, header=None)\n",
    "            test = pd.read_csv(path + \"prop/test_\"+str(i+1)+\"_\"+name, header=None)\n",
    "\n",
    "            test_y = test[test.columns[-1]]\n",
    "            test_x = test.drop([test.columns[-1]], axis=1)\n",
    "\n",
    "            wgangp_y = hybrid1[hybrid1.columns[-1]]\n",
    "            wgangp_x = hybrid1.drop([hybrid1.columns[-1]], axis=1)\n",
    "\n",
    "\n",
    "            clf = KNeighborsClassifier(5)\n",
    "            clf.fit(wgangp_x, wgangp_y)\n",
    "            pred_y = clf.predict(test_x)\n",
    "            report = classification_report(test_y, pred_y, output_dict=True)\n",
    "            disp = plot_confusion_matrix(clf, test_x, test_y,\n",
    "                                          display_labels=[0, 1],\n",
    "                                          cmap=plt.cm.Blues)\n",
    "            precision = report['macro avg']['precision']\n",
    "            recall = report['macro avg']['recall']\n",
    "            score = report['accuracy']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            auc = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "\n",
    "            tp = disp.confusion_matrix[1][1]\n",
    "            fp = disp.confusion_matrix[0][1]\n",
    "            fn = disp.confusion_matrix[1][0]\n",
    "            tn = disp.confusion_matrix[0][0]\n",
    "            results[name][i+1][\"wgangp\"]['knn'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "\n",
    "            clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=len(wgangp_x.columns))\n",
    "            clf.fit(wgangp_x, wgangp_y)\n",
    "            pred_y = clf.predict(test_x)\n",
    "            report = classification_report(test_y, pred_y, output_dict=True)\n",
    "            disp = plot_confusion_matrix(clf, test_x, test_y,\n",
    "                                          display_labels=[0, 1],\n",
    "                                          cmap=plt.cm.Blues)\n",
    "            precision = report['macro avg']['precision']\n",
    "            recall = report['macro avg']['recall']\n",
    "            score = report['accuracy']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            auc = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "\n",
    "            tp = disp.confusion_matrix[1][1]\n",
    "            fp = disp.confusion_matrix[0][1]\n",
    "            fn = disp.confusion_matrix[1][0]\n",
    "            tn = disp.confusion_matrix[0][0]\n",
    "            results[name][i+1][\"wgangp\"]['rf'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "\n",
    "\n",
    "            clf = DecisionTreeClassifier(max_depth=5)\n",
    "            clf.fit(wgangp_x, wgangp_y)\n",
    "            pred_y = clf.predict(test_x)\n",
    "            report = classification_report(test_y, pred_y, output_dict=True)\n",
    "            disp = plot_confusion_matrix(clf, test_x, test_y,\n",
    "                                          display_labels=[0, 1],\n",
    "                                          cmap=plt.cm.Blues)\n",
    "            precision = report['macro avg']['precision']\n",
    "            recall = report['macro avg']['recall']\n",
    "            score = report['accuracy']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            auc = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "\n",
    "            tp = disp.confusion_matrix[1][1]\n",
    "            fp = disp.confusion_matrix[0][1]\n",
    "            fn = disp.confusion_matrix[1][0]\n",
    "            tn = disp.confusion_matrix[0][0]\n",
    "            results[name][i+1][\"wgangp\"]['dt'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "\n",
    "\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(wgangp_x, wgangp_y)\n",
    "            pred_y = clf.predict(test_x)\n",
    "            report = classification_report(test_y, pred_y, output_dict=True)\n",
    "            disp = plot_confusion_matrix(clf, test_x, test_y,\n",
    "                                          display_labels=[0, 1],\n",
    "                                          cmap=plt.cm.Blues)\n",
    "            precision = report['macro avg']['precision']\n",
    "            recall = report['macro avg']['recall']\n",
    "            score = report['accuracy']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            auc = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "\n",
    "            tp = disp.confusion_matrix[1][1]\n",
    "            fp = disp.confusion_matrix[0][1]\n",
    "            fn = disp.confusion_matrix[1][0]\n",
    "            tn = disp.confusion_matrix[0][0]\n",
    "            results[name][i+1][\"wgangp\"]['nb'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "\n",
    "\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(wgangp_x, wgangp_y)\n",
    "            pred_y = clf.predict(test_x)\n",
    "            report = classification_report(test_y, pred_y, output_dict=True)\n",
    "            disp = plot_confusion_matrix(clf, test_x, test_y,\n",
    "                                          display_labels=[0, 1],\n",
    "                                          cmap=plt.cm.Blues)\n",
    "            precision = report['macro avg']['precision']\n",
    "            recall = report['macro avg']['recall']\n",
    "            score = report['accuracy']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            auc = roc_auc_score(test_y, clf.predict_proba(test_x)[:, 1])\n",
    "\n",
    "            tp = disp.confusion_matrix[1][1]\n",
    "            fp = disp.confusion_matrix[0][1]\n",
    "            fn = disp.confusion_matrix[1][0]\n",
    "            tn = disp.confusion_matrix[0][0]\n",
    "            results[name][i+1][\"wgangp\"]['lr'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "            append_record(results)\n",
    "\n",
    "dict1 = results\n",
    "results = {}\n",
    "\n",
    "nan = 0\n",
    "for nam in dict1 :\n",
    "      if(len(dict1[nam]) < 10) :\n",
    "          print(\"there are not 10 iterations for \" + nam + \", hence, results will not be calculated for it\")\n",
    "          continue\n",
    "      results[nam] = {\"wgangp\":{}}\n",
    "      for itr in dict1[nam] :\n",
    "            for typ in dict1[nam][itr] :\n",
    "                  for classifier in dict1[nam][itr][typ] :\n",
    "                        if typ not in results[nam] :\n",
    "                              results[nam][typ] = {}\n",
    "                        if classifier not in results[nam][typ] :\n",
    "                              results[nam][typ][classifier] = [[], [], [], [], [], [], [], [], [], [], [], []]\n",
    "\n",
    "                        i = 0\n",
    "                        for value in dict1[nam][itr][typ][classifier] :\n",
    "                              results[nam][typ][classifier][i].append(value)\n",
    "                              i += 1\n",
    "\n",
    "final_results = {}\n",
    "for name in results :\n",
    "      final_results[name] = {\"wgangp\":{}}\n",
    "      for typ in results[name] :\n",
    "            for classifier in results[name][typ] :\n",
    "                  final_results[name][typ][classifier] = []\n",
    "                  for values in results[name][typ][classifier] :\n",
    "                        final_results[name][typ][classifier].append(sum(values)/len(values))\n",
    "\n",
    "with open('./results/averaged_results_wgangp.txt', 'w') as f:\n",
    "    f.write(str(final_results))\n",
    "\n",
    "print(\"Results are printed in below order.\")\n",
    "print(\"accuracy, precision, recall, f1-score, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1\")\n",
    "dict1 = final_results\n",
    "for name in sorted(dict1) :\n",
    "      print(name)\n",
    "      for typ in dict1[name] :\n",
    "            for classifier in dict1[name][typ] :\n",
    "                  print(classifier)\n",
    "                  for value in dict1[name][typ][classifier] :\n",
    "                        print(round(value, 4), end = \" \")\n",
    "                  print()\n",
    "            print()\n",
    "      print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da2b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d660b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
