{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6177,
     "status": "ok",
     "timestamp": 1678715178017,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "8VZem5rFafnQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from numpy import savetxt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import chi2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6781,
     "status": "ok",
     "timestamp": 1677837133964,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "5k6Yj3Lt4sV6",
    "outputId": "25f761b4-857e-4cf6-a627-215db1c07acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4145,
     "status": "ok",
     "timestamp": 1678715184577,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "71XmBE5ruV6M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from scipy.io.arff import loadarff \n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import chi2, RFE\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, StackingClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score, average_precision_score, \\\n",
    "    mean_squared_error, r2_score, brier_score_loss\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import pearsonr\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import logging\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1678715269611,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "6Isz4uSwak1X",
    "outputId": "9a177d77-08ec-467b-e492-4c65ee908248"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loc</th>\n",
       "      <th>v(g)</th>\n",
       "      <th>ev(g)</th>\n",
       "      <th>iv(g)</th>\n",
       "      <th>n</th>\n",
       "      <th>v</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>i</th>\n",
       "      <th>...</th>\n",
       "      <th>IOCode</th>\n",
       "      <th>IOComment</th>\n",
       "      <th>IOBlank</th>\n",
       "      <th>locCodeAndComment</th>\n",
       "      <th>uniq_Op</th>\n",
       "      <th>uniq_Opnd</th>\n",
       "      <th>total_Op</th>\n",
       "      <th>total_Opnd</th>\n",
       "      <th>branchCount</th>\n",
       "      <th>defects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>282.6</td>\n",
       "      <td>887.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7.49</td>\n",
       "      <td>114.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>14.8</td>\n",
       "      <td>17.4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.4</td>\n",
       "      <td>622.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.69</td>\n",
       "      <td>14.26</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>123.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>53.3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>471.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>7.65</td>\n",
       "      <td>57.50</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>43.6</td>\n",
       "      <td>15.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>56.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.6</td>\n",
       "      <td>2837.89</td>\n",
       "      <td>0.08</td>\n",
       "      <td>41.47</td>\n",
       "      <td>11.39</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>14.7</td>\n",
       "      <td>21.4</td>\n",
       "      <td>36.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>316.9</td>\n",
       "      <td>2606.45</td>\n",
       "      <td>0.08</td>\n",
       "      <td>12.52</td>\n",
       "      <td>42.34</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>17.3</td>\n",
       "      <td>51.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   loc  v(g)  ev(g)  iv(g)      n        v     l      d       i  \\\n",
       "0           0  56.0  18.2    1.1    1.5  282.6   887.70  0.10   7.49  114.25   \n",
       "1           1  39.7   1.6    7.6    1.0  252.4   622.68  0.36  12.69   14.26   \n",
       "2           2  53.3   7.2    9.9    1.0   56.9   471.91  0.14   7.65   57.50   \n",
       "3           3  56.5   6.2    1.0    1.0  146.6  2837.89  0.08  41.47   11.39   \n",
       "4           4  35.3   3.3   11.6    1.0  316.9  2606.45  0.08  12.52   42.34   \n",
       "\n",
       "   ...  IOCode  IOComment  IOBlank  locCodeAndComment  uniq_Op  uniq_Opnd  \\\n",
       "0  ...       1          5       10                  0     11.7       14.8   \n",
       "1  ...       2          1        6                  0      2.7        1.7   \n",
       "2  ...       8          6        4                  0     13.8      159.8   \n",
       "3  ...       3          8        9                  0      1.1       14.7   \n",
       "4  ...       3          2        1                  0      1.0       10.8   \n",
       "\n",
       "   total_Op  total_Opnd  branchCount  defects  \n",
       "0      17.4        12.4         23.4    False  \n",
       "1     123.3         0.0         73.2    False  \n",
       "2      43.6        15.5          7.0    False  \n",
       "3      21.4        36.9          6.4    False  \n",
       "4      17.3        51.8         13.0    False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cm1 = pd.read_csv(\"./data/cm1/cm1-CTGAN.csv\")\n",
    "df_cm1.head()\n",
    "df=df_cm1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1678715272009,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "k-HQ1YC2akYY",
    "outputId": "f2092148-74ed-43ca-f9c3-8f84a230926c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "X = df.iloc[:, :-1]\n",
    "X.head()\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678715272877,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "_1NchT7kcTBy",
    "outputId": "3827cf93-9edc-4bdc-97a3-5723f7aa9982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678715274970,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "C3oOuK9tazUx",
    "outputId": "32ad0722-055b-4834-e786-b19667e2ee0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    916\n",
       "True      84\n",
       "Name: defects, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 111239,
     "status": "ok",
     "timestamp": 1678715386808,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "hJ9j-vZt9hRv",
    "outputId": "d8fc3858-372a-41dd-ccf6-a5b724e56ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel=\"linear\")\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    step=1,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(\n",
    "    #range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n",
    "   # rfecv.grid_scores_,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1678715395116,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "DrD35U4b9hXq"
   },
   "outputs": [],
   "source": [
    "rfecv.n_features_ = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8991,
     "status": "ok",
     "timestamp": 1678715405806,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "UgFiiRcF9lFU",
    "outputId": "62a4d717-a7ad-4b53-e875-691d74eb5c0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 4, 1, 1, 3, 1, 6, 1, 1, 1, 1, 1]),\n",
       " array([ True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "         True, False,  True,  True, False,  True, False,  True,  True,\n",
       "         True,  True,  True]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = RFE(estimator = RandomForestClassifier(random_state=0), n_features_to_select=rfecv.n_features_, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "selector.ranking_, selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678715405806,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "_IgxCElv9lHd",
    "outputId": "c8bd2f25-d418-49c9-b9f2-0cbf30e754be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 10, 13, 15]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, x in enumerate(selector.support_) if not x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678715405807,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "sYEspZPB9n1K"
   },
   "outputs": [],
   "source": [
    "X = X.drop(X.columns[[i for i, x in enumerate(selector.support_) if not x]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1678715472271,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "HEbBJyyS9nv5"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1678715473349,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "vdEGI0UcazSU"
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1678715474799,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "baMG2QfYazPy",
    "outputId": "a7e529e6-d864-498d-9a27-0a24906a8e61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.34169746e-01, -3.79971989e-01, -2.85297268e-01, ...,\n",
       "        -4.41634013e-01, -4.50326207e-01, -4.37779376e-01],\n",
       "       [-5.35475142e-01, -4.10690827e-01, -3.29165274e-01, ...,\n",
       "        -4.42954086e-01, -4.52319286e-01, -4.55481171e-01],\n",
       "       [ 3.91355850e-01,  5.00917534e-02,  2.19184797e-01, ...,\n",
       "         2.89685946e-01,  3.94739443e-01,  7.55726713e-02],\n",
       "       ...,\n",
       "       [-2.62878871e-04, -1.80299537e-01, -2.19495260e-01, ...,\n",
       "        -6.01331681e-02, -2.38072231e-02, -1.89954250e-01],\n",
       "       [-4.17989523e-01, -4.10690827e-01, -3.29165274e-01, ...,\n",
       "        -3.10946873e-01, -3.12803731e-01, -4.55481171e-01],\n",
       "       [-3.00503904e-01, -2.57096634e-01, -3.29165274e-01, ...,\n",
       "        -2.44943266e-01, -1.93218969e-01, -2.78463223e-01]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1678715476471,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "vjirruQRazNY",
    "outputId": "b7d37bbd-35bc-41e6-c6a5-5585de7fc298"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10880, 16)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJk6hPPxbABW"
   },
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "c4-V0U55azKb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 8777, 1: 2103})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "a3kfQm1xazHi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 8777, 1: 8777})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgAUlEQVR4nO3df5RcZZ3n8fc3nY7dCpukSYNJOkyCMFkgZojbcHRBVOIxwEgSctYcOTsuLqzRNTr4K5KwR0TmB8GM4jgCY5CsOOugfda2k8EBJhNwFY4KHcJ0YDAHHRzSHQ5pEhONNqTT/d0/blW6urpu1a3f91Z9Xuf06apbt6oeLulvPfV9vs/zmLsjIiLJM63eDRARkdIogIuIJJQCuIhIQimAi4gklAK4iEhCTa/lm82ZM8cXLlxYy7cUEUm83bt3v+LundnHaxrAFy5cSH9/fy3fUkQk8czs33MdVwpFRCShFMBFRBJKAVxEJKFqmgPPZXR0lMHBQV599dV6NyVUW1sbXV1dtLa21rspIiIn1T2ADw4Ocuqpp7Jw4ULMrN7NmcLdOXToEIODgyxatKjezREROanuKZRXX32V0047LZbBG8DMOO2002L9DUFEYmygB+5YArfMCn4P9FTspeveAwdiG7zT4t4+EYmpgR74hz+F0ZHg/tH9wX2ApWvLfvm698BFRBrWrlsngnfa6EhwvAIKBnAzazOzJ8zsX8zsWTP7Qur4LWY2ZGZPp36urEiL6uShhx5i8eLFnH322WzevLnezRGRRnB0sLjjRYqSQnkNuMzdj5lZK/CYmT2YeuwOd/+rirSkjsbGxli/fj07d+6kq6uLCy+8kJUrV3LeeefVu2kikmQzu4K0Sa7jFVCwB+6BY6m7ramfum3j07dniIs3P8KijT/g4s2P0LdnqOzXfOKJJzj77LM566yzmDFjBu9///vZvn17BVorIk1t+c3Q2j75WGt7cLwCIuXAzazFzJ4GDgI73f1nqYc+ZmYDZrbNzGZXpEV59O0ZYlPvXoaOjODA0JERNvXuLTuIDw0NsWDBgpP3u7q6GBoq/4NBRJrc0rVw1Vdh5gLAgt9XfbUiA5gQsQrF3ceAC8xsFvB9M1sC3A38GUFv/M+ALwHXZT/XzNYB6wDOPPPMshq75eF9jIyOTTo2MjrGlof3sXrZ/JJfN9e+oKo8EZGKWLq2YgE7W1FVKO5+BPghcLm7v+zuY+4+DtwDXBTynK3u3u3u3Z2dU1ZDLMqBIyNFHY+qq6uL/fsn8lSDg4PMmzevrNcUEam2KFUonameN2bWDrwb+LmZzc047Wrgmaq0MMO8We1FHY/qwgsv5Pnnn+eFF17g+PHjfOc732HlypVlvaaISLVF6YHPBR41swHgSYIc+APAF81sb+r4u4BPVrGdAGxYsZj21pZJx9pbW9iwYnFZrzt9+nS+9rWvsWLFCs4991zWrl3L+eefX9ZriohUW8EcuLsPAMtyHP9AVVqURzrPveXhfRw4MsK8We1sWLG4rPx32pVXXsmVVya6lF1EmkwsptIXY/Wy+RUJ2CIiSaep9CIiCaUALiKSUArgIiIJpQAuIpJQCuAiIgmlAA5cd911nH766SxZsqTeTRERiUwBHPjgBz/IQw89VO9miIgUJXkBvAr7y1166aV0dHSU3zYRkRpK1kSeKu8vJyKSJMnqgVd5fzkRkSRJVgCv8v5yIiJJkqwAHraPXIX2lxMRSZJkBfAq7S93zTXX8La3vY19+/bR1dXFvffeW9briYjUQrIGMdMDlbtuDdImM7uC4F3mAOb9999fgcaJiNRWsgI4VHV/ORGRJElWCkXyq0KNvIjEVyx64O4e613gc+1aHztxq5Ef6Kl4qktEJqt7D7ytrY1Dhw7FNki6O4cOHaKtra3eTckvTjXy6Q+To/sBn/gw0TcCkYoq2AM3szbgR8DrUuf/X3f/vJl1AN8FFgK/Ata6+6+LbUBXVxeDg4MMDw8X+9SaaWtro6sr5qWKcaqRz/dhol64SMVESaG8Blzm7sfMrBV4zMweBNYAu9x9s5ltBDYCNxbbgNbWVhYtWlTs0yTbzK5UjzfH8VqL04eJSAMrmELxwLHU3dbUjwOrgPtSx+8DVlejgRJRlWrkS6IJVyI1ESkHbmYtZvY0cBDY6e4/A85w95cAUr9PD3nuOjPrN7P+OKdJEm/pWrjqqzBzAWDB76u+Wp+URZw+TKS+VBlVVZECuLuPufsFQBdwkZlF3vnA3be6e7e7d3d2dpbYTCkoTlUfcfowkfrRYHbVFVVG6O5HzOyHwOXAy2Y2191fMrO5BL1zqYe4lRCm31cBu7lpMLvqCvbAzazTzGalbrcD7wZ+DuwArk2ddi2wvUptlELiVEIokqbB7KqL0gOfC9xnZi0EAb/H3R8ws58APWZ2PfAi8L4qtlPy0R+KxFGcKqMaVMEA7u4DwLIcxw8By6vRKClS0v9Q4pS/l8pZfvPk1B5oMLvC6j4TUyogyVUfuQa6ej8Ety/SYFfSaTC76mKxFoqUqUrL7NZErvw9wMjh+g/ESvk0mF1VCuCNIql/KPny9KpYEMlLKZSkabSJEYXy9BqIFQmlAJ4kjTgxIlf+PlNSBmJF6kABPEkasd47PdDV3jH1saQMxIrUiQJ4kjRqvffStXDjC7DmHlUsiBRBg5hJkvR670KSOhArUifqgSdJkuu9RaTiFMCTRBMjRCSDUihJk6Q0g6bIi1SVArhURxyXuBVpMEqhSHU0YsmjSMwogEt1NGrJo0iMKIBLdWhjY5GqUwCX6lDJo0jVKYBLdajkUaTqVIUi1ZOkkkeRBIqyqfECM3vUzJ4zs2fN7IbU8VvMbMjMnk79XFn95oqISFqUHvgJ4NPu/pSZnQrsNrOdqcfucPe/ql7zREQkTJRNjV8CXkrd/q2ZPQfMr3bDREQkv6IGMc1sIcEO9T9LHfqYmQ2Y2TYzmx3ynHVm1m9m/cPDw+W1thk02o47IlI1kQO4mZ0CfA/4hLv/BrgbeBNwAUEP/Uu5nufuW9292927Ozs7y29xI2vEHXdEpGoiBXAzayUI3t92914Ad3/Z3cfcfRy4B7ioes1sEpp+LiJFiFKFYsC9wHPu/uWM43MzTrsaeKbyzWsymn4uIkWIUoVyMfABYK+ZPZ06dhNwjZldADjwK+DDVWhfc2n0HXdEpKKiVKE8BliOh/6x8s1pcstvnrwEK2j6uYiE0lT6ONH0cxEpgqbSx42mn4tIROqBi4gklAK4iEhCKYCLiCSUAriISEIpgIuIJJQCuIhIQimAi4gklAK4iEhCKYCLiCSUArhIqbT5htSZptKLDPQEa64fHQxWflx+c+HlDNKbb6QXHktvvgFaCkFqRj1waW6l7oKkzTckBhTApbmVGoi1+YbEgAK4NLdSA3HYJhvafENqSAG8ljToFT+lBuLlNwebbWTS5htSYwrgtaId5+Op1ECszTckBlSFUiv5cq36o6+f9LXPV4Uy0AMP3ggjh4P77R1wxe3afEPqLsqu9AvM7FEze87MnjWzG1LHO8xsp5k9n/o9u/rNTTANeiXTQA/0fXQieENwe/t6fXuSuouSQjkBfNrdzwXeCqw3s/OAjcAudz8H2JW6L2E06FUd5Y4rFEpt7boVxkenPm/suEoGpe4KBnB3f8ndn0rd/i3wHDAfWAXclzrtPmB1ldrYGDToVXmVGFcoVEaY7xuSvj1JnRU1iGlmC4FlwM+AM9z9JQiCPHB6yHPWmVm/mfUPDw+X2dwE06BX5VViMk2h1Fa+b0j69iR1FnkQ08xOAb4HfMLdf2NmkZ7n7luBrQDd3d1eSiMbhga9KqsS4wozu1I9+Czts4OUTK7HAFpm6NuT1F2kHriZtRIE72+7e2/q8MtmNjf1+FzgYHWa2EBUB15ZlRhXyJXamtYKx4+FB+/2Dlh1pz6Mpe6iVKEYcC/wnLt/OeOhHcC1qdvXAtsr37wGojrwyqvEuEKu1NbrTg0GKbPNXAC3HIUbX1Dwllgw9/xZDTO7BPgxsBcYTx2+iSAP3gOcCbwIvM/dD+d8kZTu7m7v7+8vt83JFPZ1fOYC+OQztW9PoyhlJcFCbpkF5Pq7MLjlSHmvLVICM9vt7t3ZxwvmwN39MSAs4b283IY1DdWBV0c1xhXC8uIatJSY0VT6WlEdeHKo5FMSQgG8VhQUaqfcwWKVfEpCaC2UWomy5oaUr1I75ajkUxJAAbyWFBSqr9RFw6oxGCpSZQrg0lhKGSx+4FPQv42TlSfa31ISQjlwSa5cue5iB4sHeiYH7zTtbykJoAAuyRQ2Meqc9xQ3WLzrVnLXfKMST4k9pVCk/qLmnzPPs2ngY5MfHx2B5/8pqBiJms/OF6RV4ikxpwAu9RW1aiT7vOzgnXZ0sLjB4rBJO5hKPCX2lEKR+oq6JGyu83Ipttecqz4fg+7rNIApsaceuNRX1KqRKPnoUiZGqT5fEkwBXOor6rojYedZC/h4eYFX9fmSUEqhSH1FXWIg7Lyr/zZYIfCTzygIS9NRAJf6irruiNYnEZmi4HrgldTU64GLiJQobD1w9cBFRBJKg5hSWbkm5QA8eCOMpDZsau+AK25X+kOkTArgUjm5JuVsXw9jJ5jYjY8gkG9fH9xWEBcpWZRNjbeZ2UEzeybj2C1mNmRmT6d+rqxuMyURck22GTvOpOCdeVyLRYmUJUoO/JvA5TmO3+HuF6R+/rGyzWpy5e4oUy/FLv6kxaJEylIwgLv7j4C8u81LBQ30QN9HJ6+y1/fRZATxYqexa7EokbKUU4XyMTMbSKVYZoedZGbrzKzfzPqHh4fLeLsm8eCNMD46+dj4aHA87nJNtmmZQc5/Zi0ztFiUSJlKDeB3A28CLgBeAr4UdqK7b3X3bnfv7uzsLPHtmshIyJedsONxkmuyzao7Yc3Xg8qTtPaO4LgGMEXKUlIViru/nL5tZvcAD1SsRZJsYeuKKFiLVFxJPXAzm5tx92rgmbBzpUiZPdVsSciDi0jNRCkjvB/4CbDYzAbN7Hrgi2a218wGgHcBn6xyO5vHFbeHP5av7C6plSsiUrKCKRR3vybH4Xur0BaBINXQ+6Hcj4WV3UXd1SYuom6hJiJ5aS2UOJq5IOR4SNld1F1t4iBsM2J9YxApmgJ4HEVdIzst6q42cZCkDxuRmFMAj6Ni174O65nHcaJMkj5sRGJOATyulq4NdplZszW437sufHCy2B57PVXjw0YDuNKkFMDjLGq+OEm71VT6w0Y5dWli2pEnbjIrNGwa+NjUc2YuCHrnSVXJKpQ7loRsipzwaySSIWxHHq0HHifZ5YC5gjckP19cyV3glVOXJqYAXmN9e4bY8vA+DhwZYd6sdjasWMzqZfODB3NVaOQSx8HJepnZFdID1zWSxqcceA317RliU+9eho6M4MDQkRE29e6lb89QcEKUXmNcByfrJUkDuCIVpgBeQ1se3sfI6OS0yMjoGFse3hfcCes1WguxH5yslyQN4IpUmFIoNXTgSO70yMnjy2+enAOHoDepgJRfJXPqIgmiHngNzZvVnv+4epMiUgT1wGtow4rFbOrdOymN0t7awoYViydOUm9SRCJSD7yasmYIrm55nNvWvJn5s9oxYP6sdm5b8+aJKpQ8z9XEFBHJpok81ZJd0w3R89nlPFdEGk7YRB71wKulnFX3tGKfiESgAF4t5cwQ1OxCEYlAAbxayll1L0nLw4pI3SiAV0s5MwQ1u1BEIoiyqfE2MztoZs9kHOsws51m9nzq9+zqNjOByqnpVj24iERQsArFzC4FjgHfcvclqWNfBA67+2Yz2wjMdvcbC71ZYqpQtOmuiMRIyVUo7v4j4HDW4VXAfanb9wGry21gbGiDABFJiFJz4Ge4+0sAqd+nh51oZuvMrN/M+oeHh0t8uxpSCV9+mmAkEhtVH8R0963u3u3u3Z2dndV+u/KphC9crb6d6ENCJJJSA/jLZjYXIPX7YOWaVIJK/sGrhC9cLb6dKIUlElmpAXwHcG3q9rXA9so0pwT5/uBLCez1LuGLc++zFt9OlMISiazgaoRmdj/wTmCOmQ0Cnwc2Az1mdj3wIvC+ajYyr7A/+AdvhBMjE4+lAzuEV5Skq09GR4JNFHwsKOGrVRVK9hooUdpcS7XYvkwpLJHIolShXOPuc9291d273P1edz/k7svd/ZzU7+wqldoJ+8MeOVxcT+6BT0HvuokA5WMTPe9aBc+49z5r8e1EKSyRyJI/E7PYP+xcAX+gB/q3AVk18eUGz2LTIXHvfdZiglG9U1giCZL4DR2efNPH+U9PfTb6J1GugL/rVqYE77RSgudAT5DCGcn4YnJ0P/R+KDh+xe25g14Sdliv9oYT6dfWRCqRghIdwPv2DLHpyT/gX6cBFuEJYT25fEE6SvDMnLnZPhuOH4Ox47nPHTkcntcO2xOz2Xqf2pVIJJJEp1DSu7wP+Zzwk9o7AAt+T28P8tzZ6YzQIG2Fg2d2FczI4fDgnRaWmtEaKCJShET3wNO7uX/xxFq+0noX03L1wl89ShBYf83JNEl2dUeuni8G3dcVDp65Bh6jCOv1q/cpIhElugee3s19x/gl4Sd5egPhPAOUuXq+a7bCe78c/rrpAcpcOesoMnv9Az3wl/Pglpmpn1lBVYyISB6J7oFn7vJ+wOfQZa8U9wKZwbeYnm+uPSuLkZnXHuiBvo/A+FjGCQ799wY3832IiEhTS3QPfPWy+Sd3ed9yYi0jvK74F7l9UfGzHh+8sXDwttTI6swF0H19eF57161ZwTvD7m9Ga4+INKVE98AhCOKrl80HLoOBZbDrVvzoIGNuTLfxwi+QLvWLOutxoGdyeWAYH482izNfBYyHBHYRERLeAw/jwG94Pa95S3FPjDJxp5iJPUf3Q99H8/fs85Qpjts0Lt78CIs2/oCLNz9C356h6O8tIg2vcQJ4RjnfNJwOO4ZhHBo/hXE3TnjE/9RCE3eKndgzPhqkXMIsvxmmTf2gceD+seUMHRnBgaEjI2zq3asgLiInNU4Az1HON8NOMEIbZ732bT41+pFoOfLMHvEDn4IvdASVIV/ogPtWEjpjM5/MlEv29HqA1X8LM96Q8QSj1y7nfx3/75NfZnSMLQ/vK/79RaQhJT4HflJIz3ieHQJgZ8s7+MBbFnLhL/8mfMZkaztPvunjfGLzI3z42J18YPo/T0zw9DF44f+V3r47lkDHWZNfI51iWX0X3HRg0umf2fiDnC+Trn0XEWmcAB6yjsgBP435s9rZsGIxFy67HPjwxIMnp8AHz/PREbp3f5ZHmUZry3ik2fmRHd2fu2Y8nWLJGuicN6udoRzBOl37Pok2YRZpSolMofTtGZo6uBeyil3Xf7mNxzdelqpUybJ0LZzznpN3DTCDGTaOVTR6F5CjqmXDisW0t07Ojbe3trBhxeLJJ2oHG5GmlbgA3rdniE29e6cO7o1dXNo6IjGttc6scTdg/qx2blvz5qkfRHFfQ1xEqiZxKZT0AlaZ0oN7qzeGzKYMSzEM9OA+VtlUSSnaO3IenqhxzyPua4iLSNUkLoCHDeKFDu6FbVP24k9hz98VFbydyavWZt8vybSWYH3wUiVhDXERqYqyAriZ/Qr4LTAGnHD37ko0Kp+iBvcgPMWw+38HsyWLkB2sjWCyzTR3SiovbJkBq+4MTfP07Rliy8P7OHBkhHmpgdgpPXKtIS7StCqRA3+Xu19Qi+ANRQzupYWlEooM3qF8HFpfX/zzuq+Hzw3nDd45c/3ZE3m0hrhI00pcCiXdA83XM83suf6kbQ5vZHjK67hTkUoTc2D0d9Gf0H19pBUG8+b6s3vhWkNcpCmVG8Ad+Cczc+Dr7r41+wQzWwesAzjzzDPLfLtA6ODeQA+v9n6cVf4qqwBeB695S5CrzgrWlSoTLOp1Wt8QeXnYonP9ItJ0yk2hXOzubwGuANab2aXZJ7j7Vnfvdvfuzs7OMt8uj4Eexns/RBuvYsbJnzaL0Yp+V30l8qlhOf3QXL+INJ2yAri7H0j9Pgh8H7ioEo0qyYM3hv7H1HRSTpj2jqLSHEXn+kWk6ZQcwM3sDWZ2avo28B7gmUo1rGhR1uiul9b2oksFI0/kEZGmVU4O/Azg+xZ0b6cDf+/uD1WkVTE37uTeQDmXPJs6FCoTjDSRR0SaVskB3N3/DfijCralNKlZlhWZVBPBuMPfjb2bq1p+ymw7Fv6ea+7JmzJJlwmmK03SZYKAgraIRJK4tVAyPbnj6xzv/Qgc3V/T6fCfP3Edb3ltK7/2U3KfMHNBwXx3vjJBEZEoEhvA+/YM8abdtzKD2laZGPCt1r8A4Kut/yPnCoiTdpzP3LzhgU+lNlGeyWMjV7N7xjpWTnts0tNVJigiUSVuIk/alof3sYpjNX9fM3j7tGe5evrjXPDH66Hl/KkLZUEQqDMHVo/uh/57J73OaXaML7XeDaOwY/wSQGWCIhJdYgP40JERouyQVg1msGH6d5m37C+BrFmQ2YtnFdBqzuenf4sdxy9RmaCIFCWxAbylzsXdb/RDuR/ItXhWAR127OSuQRrAFJGoEhvAx7yE1f8q6Dd2CrNyPVDCOtxm8PjGy8ptkog0mWQOYg708NO2G+rahNczwg03bZrY0i2tlHW4QzZ0qIjsgVRttSbSMJLRAx/ogX/4BJ5e9c/hjUZtCr9DzOAEX2m9C0bu4kDfHJ7c/1kuXPnhnOtzp78shGZ9StnQIXNDZmsBH5s6aShsMwvQ6oUiDSD+PfCBHvj+R2D0d6RjdizWNmFiwaz59gpLnvpcUCaYyoGPMQ13GByfww2jH83/QsUG00kbGRMEb5i6obH2yxRpaPHvge+6dSJAxVg7r0H/NtI787Qwzu+ZwRdPrGXH+CX8NXeV/uLZe3oe/134QGk6QC9dq/0yRRpc/HvgiQo2kwdWX2/H+ez0oDf8u7Cax9Y35H/JSb1tD34XWrgrfc3C8vHaL1OkIcQ/gCc82My3VwC4afR6xnIVzoz+Hm6ZGfzcvmjqIGMJZYknr9nym/PPFBWRRIt/AF9+Myc8JknvEjicnC4/ljNjlRHVRw7D9vWTg3ix30AyA7T2yxRpaOY1rKfu7u72/v7+op/3pzdt4q9b74rN4GWxDo2fwkz7PdMt4kbKMxfAJ1NLq9+xZGKwMlN7B8x4Q/4qFBFpCGa2O9fG8fEfxCRYJ+R/+nb+I0OJDOIddqy4dmf2unOUJZ7cIEKBWqSpxT6F8l/v+QkAT4yfW+eWlK7oD53MvL/SICISIvY98Md/eZgvTN/Gf2v550T2vovWMmPqIOPStQrYIjJF7HvgQGMHb8v4X9DeAavuVLAWkUhi3wN/asb19W7CFO6F0yIFzymw5ZqISCFl9cDN7HIz22dmvzCzjZVqVKbZNhKr3nehop309Plvjb07vPxx0TvoG7uYizc/wqKNP5i6IJaISAQlB3AzawHuBK4AzgOuMbPzKtWwuEqvf1LI7vE/5M9bb5g809KmQff19C29m029exk6MoIzsaGxgriIFKOcFMpFwC9Su9NjZt8BVgH/WomGAfzoc/+ZtyciSz/BDLrsFW5v/QbPLP1zWHlgyjlbNj8SuqGxNnQQkajKCY/zgcwZJoOpY5OY2Toz6zez/uHh4aLe4O3Tno1V+qQY7XacC3/5NzkfC9u4WBsai0gxygnguULrlAyxu29192537+7s7Czj7eIrNC0eMg0+bONibWgsIsUoJ4APAgsy7ncBU/MFDaDk1QZCFuLasGIx7a0tk45pQ2MRKVY5AfxJ4BwzW2RmM4D3Azsq06x4sbPekZoJObW3faKlDeu+vqhV/1Yvm89ta97M/FntGDB/Vju3rXmz8t8iUpSSBzHd/YSZfQx4GGgBtrn7sxVrGRG2Iivh9RyYZpOPZb+HO7jBtBy12pa1ucL09OJRZ7518qYLBRaVWr1svgK2iJQl9qsRjt88s/wAnnr+j8fOp3f8nXym5bvMs0Mc8NP4xow/4bx5/4FLX7yb0/0VDtoc9r9lQ7C/pYhIDIStRhj7AC4i0uzCAnjCqqxFRCRNAVxEJKEUwEVEEkoBXEQkoRTARUQSqqZVKGY2DPx7hV5uDvBKhV6rEeh6TNC1mEzXY7IkXo8/cPcpa5HUNIBXkpn15yqraVa6HhN0LSbT9Ziska6HUigiIgmlAC4iklBJDuBb692AmNH1mKBrMZmux2QNcz0SmwMXEWl2Se6Bi4g0NQVwEZGESlwAN7PLzWyfmf3CzDbWuz21ZmbbzOygmT2TcazDzHaa2fOp37Pr2cZaMrMFZvaomT1nZs+a2Q2p4013TcyszcyeMLN/SV2LL6SON921yGRmLWa2x8weSN1vmOuRqABuZi3AncAVwHnANWZ2Xn1bVXPfBC7POrYR2OXu5wC7UvebxQng0+5+LvBWYH3q30QzXpPXgMvc/Y+AC4DLzeytNOe1yHQD8FzG/Ya5HokK4MBFwC/c/d/c/TjwHWBVndtUU+7+I+Bw1uFVwH2p2/cBq2vZpnpy95fc/anU7d8S/KHOpwmviQeOpe62pn6cJrwWaWbWBfwx8I2Mww1zPZIWwOcD+zPuD6aONbsz3P0lCAIacHqd21MXZrYQWAb8jCa9Jql0wdPAQWCnuzfttUj5CvBZYDzjWMNcj6QF8Fybq6kOUjCzU4DvAZ9w99/Uuz314u5j7n4B0AVcZGZL6tykujGz9wIH3X13vdtSLUkL4IPAgoz7XcCBOrUlTl42s7kAqd8H69yemjKzVoLg/W13700dbupr4u5HgB8SjJc067W4GFhpZr8iSLdeZmb/hwa6HkkL4E8C55jZIjObAbwf2FHnNsXBDuDa1O1rge11bEtNmZkB9wLPufuXMx5qumtiZp1mNit1ux14N/BzmvBaALj7JnfvcveFBLHiEXf/ExroeiRuJqaZXUmQ12oBtrn7X9S3RbVlZvcD7yRYEvNl4PNAH9ADnAm8CLzP3bMHOhuSmV0C/BjYy0Se8yaCPHhTXRMzW0owKNdC0Dnrcfdbzew0muxaZDOzdwKfcff3NtL1SFwAFxGRQNJSKCIikqIALiKSUArgIiIJpQAuIpJQCuAiIgmlAC4iklAK4CIiCfX/AaM5lecI+9gBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "over = SMOTE(random_state=42)\n",
    "# transform the dataset\n",
    "X, y = over.fit_resample(X, y)\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBM_AKvJ9nTN"
   },
   "source": [
    "ADASYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "PfJUrV2Z9mWC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 8877, 0: 8777})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN \n",
    "over = ADASYN()\n",
    "# transform the dataset\n",
    "X, y = over.fit_resample(X, y)\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1678715489262,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "TNVQ1pt5bFEZ",
    "outputId": "16e40660-7aad-4520-ca16-2e1cd3bad55c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17554, 16), (17554,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1678715490614,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "nnl4ptQKbFBu"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUSwfFVibE_G"
   },
   "outputs": [],
   "source": [
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "\n",
    "for train, test in skf.split(X, y):      \n",
    "    model = Sequential()    \n",
    "    initializer = tf.initializers.he_normal()\n",
    "    weight_decay = 1e-5\n",
    "    model.add(Conv1D(32, 1, activation='relu',kernel_initializer=initializer,kernel_regularizer = regularizers.l1_l2(weight_decay,weight_decay),input_shape=(21, 1)))\n",
    "   # model.add(Conv1D(32, 1, activation='relu',input_shape=(21, 1)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Conv1D(64, 3,kernel_initializer=initializer,kernel_regularizer = regularizers.l1_l2(weight_decay,weight_decay), activation='relu', padding='same'))   \n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))  \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Conv1D(128, 3, activation='relu',kernel_initializer=initializer,kernel_regularizer = regularizers.l1_l2(weight_decay,weight_decay), padding='same'))  \n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))   \n",
    "   # model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Conv1D(64, 3, activation='relu',kernel_initializer=initializer,kernel_regularizer = regularizers.l1_l2(weight_decay,weight_decay), padding='same'))   \n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    " #   model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    " #   model.add(BatchNormalization())    \n",
    "   # model.add(Conv1D(32, 3, activation='relu',input_shape=(21, 1)))\n",
    "   # model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "   # model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    history = model.fit(X[train], y[train], batch_size = 32, epochs = 200, validation_data=(X[test], y[test]), verbose = 2)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X[test], y[test], verbose = 1)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    acc_train_his.append(history.history['accuracy'])\n",
    "    acc_val_his.append(history.history['val_accuracy'])\n",
    "    \n",
    "    loss_train_his.append(history.history['loss'])\n",
    "    loss_val_his.append(history.history['val_loss'])\n",
    "    \n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1343101,
     "status": "ok",
     "timestamp": 1678720276318,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "8y07PMt3uNo-",
    "outputId": "79f25409-7637-4985-b8e9-4e03bd63724b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 7s - loss: 0.6593 - precision: 0.6272 - val_loss: 0.6214 - val_precision: 0.6667 - 7s/epoch - 6ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 5s - loss: 0.6449 - precision: 0.6644 - val_loss: 0.6178 - val_precision: 0.6514 - 5s/epoch - 4ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 4s - loss: 0.6392 - precision: 0.6600 - val_loss: 0.6208 - val_precision: 0.6674 - 4s/epoch - 4ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 4s - loss: 0.6355 - precision: 0.6665 - val_loss: 0.6172 - val_precision: 0.6876 - 4s/epoch - 4ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 4s - loss: 0.6372 - precision: 0.6496 - val_loss: 0.6068 - val_precision: 0.6758 - 4s/epoch - 4ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 5s - loss: 0.6300 - precision: 0.6704 - val_loss: 0.6078 - val_precision: 0.7009 - 5s/epoch - 4ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 4s - loss: 0.6324 - precision: 0.6729 - val_loss: 0.6039 - val_precision: 0.7027 - 4s/epoch - 4ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 4s - loss: 0.6208 - precision: 0.6627 - val_loss: 0.6015 - val_precision: 0.6760 - 4s/epoch - 4ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 4s - loss: 0.6247 - precision: 0.6597 - val_loss: 0.6028 - val_precision: 0.6707 - 4s/epoch - 4ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 4s - loss: 0.6213 - precision: 0.6596 - val_loss: 0.6054 - val_precision: 0.6994 - 4s/epoch - 4ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 5s - loss: 0.6200 - precision: 0.6600 - val_loss: 0.6030 - val_precision: 0.6733 - 5s/epoch - 4ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 5s - loss: 0.6157 - precision: 0.6587 - val_loss: 0.5933 - val_precision: 0.6711 - 5s/epoch - 4ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 5s - loss: 0.6121 - precision: 0.6591 - val_loss: 0.5934 - val_precision: 0.6580 - 5s/epoch - 4ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 5s - loss: 0.6125 - precision: 0.6692 - val_loss: 0.5957 - val_precision: 0.6853 - 5s/epoch - 4ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 5s - loss: 0.6143 - precision: 0.6785 - val_loss: 0.5908 - val_precision: 0.6860 - 5s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 5s - loss: 0.6080 - precision: 0.6787 - val_loss: 0.5877 - val_precision: 0.6931 - 5s/epoch - 4ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 5s - loss: 0.6079 - precision: 0.6724 - val_loss: 0.5873 - val_precision: 0.6677 - 5s/epoch - 4ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 5s - loss: 0.6026 - precision: 0.6851 - val_loss: 0.5869 - val_precision: 0.6698 - 5s/epoch - 4ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 5s - loss: 0.6012 - precision: 0.6796 - val_loss: 0.5892 - val_precision: 0.6771 - 5s/epoch - 4ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 5s - loss: 0.6027 - precision: 0.6909 - val_loss: 0.5894 - val_precision: 0.6724 - 5s/epoch - 4ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 5s - loss: 0.5993 - precision: 0.6854 - val_loss: 0.5892 - val_precision: 0.6663 - 5s/epoch - 4ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 5s - loss: 0.5958 - precision: 0.6878 - val_loss: 0.5793 - val_precision: 0.6827 - 5s/epoch - 4ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 5s - loss: 0.5945 - precision: 0.6865 - val_loss: 0.5829 - val_precision: 0.6756 - 5s/epoch - 4ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 5s - loss: 0.5924 - precision: 0.6923 - val_loss: 0.5834 - val_precision: 0.6840 - 5s/epoch - 4ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 5s - loss: 0.5943 - precision: 0.6938 - val_loss: 0.5832 - val_precision: 0.6855 - 5s/epoch - 4ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 5s - loss: 0.5884 - precision: 0.6932 - val_loss: 0.5773 - val_precision: 0.6884 - 5s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5901 - precision: 0.6869 - val_loss: 0.5770 - val_precision: 0.6857 - 5s/epoch - 4ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5876 - precision: 0.6866 - val_loss: 0.5918 - val_precision: 0.7121 - 5s/epoch - 4ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5877 - precision: 0.6937 - val_loss: 0.5719 - val_precision: 0.6965 - 5s/epoch - 4ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 5s - loss: 0.5832 - precision: 0.6985 - val_loss: 0.5729 - val_precision: 0.6903 - 5s/epoch - 4ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 5s - loss: 0.5828 - precision: 0.6991 - val_loss: 0.5720 - val_precision: 0.7026 - 5s/epoch - 4ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 5s - loss: 0.5797 - precision: 0.7030 - val_loss: 0.5780 - val_precision: 0.6946 - 5s/epoch - 4ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 5s - loss: 0.5815 - precision: 0.7037 - val_loss: 0.5749 - val_precision: 0.7026 - 5s/epoch - 4ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 5s - loss: 0.5793 - precision: 0.7084 - val_loss: 0.5731 - val_precision: 0.6930 - 5s/epoch - 5ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 5s - loss: 0.5756 - precision: 0.7084 - val_loss: 0.5644 - val_precision: 0.6942 - 5s/epoch - 4ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 5s - loss: 0.5728 - precision: 0.7086 - val_loss: 0.5652 - val_precision: 0.7104 - 5s/epoch - 4ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 5s - loss: 0.5742 - precision: 0.7085 - val_loss: 0.5636 - val_precision: 0.6981 - 5s/epoch - 4ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 5s - loss: 0.5733 - precision: 0.7063 - val_loss: 0.5610 - val_precision: 0.7001 - 5s/epoch - 4ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 5s - loss: 0.5697 - precision: 0.7112 - val_loss: 0.5629 - val_precision: 0.7065 - 5s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 5s - loss: 0.5668 - precision: 0.7121 - val_loss: 0.5538 - val_precision: 0.6966 - 5s/epoch - 5ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5538 - precision: 0.6966\n",
      "Score for fold 1: loss of 0.5537999272346497; precision of 69.65944170951843%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 6s - loss: 0.6627 - precision_1: 0.6214 - val_loss: 0.6359 - val_precision_1: 0.6599 - 6s/epoch - 6ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 5s - loss: 0.6449 - precision_1: 0.6537 - val_loss: 0.6082 - val_precision_1: 0.6899 - 5s/epoch - 4ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 4s - loss: 0.6375 - precision_1: 0.6587 - val_loss: 0.5928 - val_precision_1: 0.7189 - 4s/epoch - 4ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 5s - loss: 0.6303 - precision_1: 0.6607 - val_loss: 0.6036 - val_precision_1: 0.6899 - 5s/epoch - 4ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 5s - loss: 0.6278 - precision_1: 0.6657 - val_loss: 0.6005 - val_precision_1: 0.7011 - 5s/epoch - 4ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 5s - loss: 0.6276 - precision_1: 0.6619 - val_loss: 0.6042 - val_precision_1: 0.7092 - 5s/epoch - 4ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 5s - loss: 0.6217 - precision_1: 0.6666 - val_loss: 0.6012 - val_precision_1: 0.7181 - 5s/epoch - 4ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 5s - loss: 0.6225 - precision_1: 0.6635 - val_loss: 0.5971 - val_precision_1: 0.6954 - 5s/epoch - 4ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 5s - loss: 0.6179 - precision_1: 0.6662 - val_loss: 0.5908 - val_precision_1: 0.6960 - 5s/epoch - 4ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 5s - loss: 0.6187 - precision_1: 0.6665 - val_loss: 0.5918 - val_precision_1: 0.6883 - 5s/epoch - 4ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 5s - loss: 0.6148 - precision_1: 0.6604 - val_loss: 0.5863 - val_precision_1: 0.6990 - 5s/epoch - 4ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 5s - loss: 0.6138 - precision_1: 0.6696 - val_loss: 0.5986 - val_precision_1: 0.7172 - 5s/epoch - 4ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 5s - loss: 0.6131 - precision_1: 0.6653 - val_loss: 0.5863 - val_precision_1: 0.7013 - 5s/epoch - 5ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 5s - loss: 0.6102 - precision_1: 0.6755 - val_loss: 0.5843 - val_precision_1: 0.6948 - 5s/epoch - 5ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 5s - loss: 0.6079 - precision_1: 0.6739 - val_loss: 0.5858 - val_precision_1: 0.6644 - 5s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 5s - loss: 0.6067 - precision_1: 0.6684 - val_loss: 0.5824 - val_precision_1: 0.6951 - 5s/epoch - 4ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 5s - loss: 0.6052 - precision_1: 0.6743 - val_loss: 0.5776 - val_precision_1: 0.6860 - 5s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 5s - loss: 0.6040 - precision_1: 0.6707 - val_loss: 0.5853 - val_precision_1: 0.6590 - 5s/epoch - 5ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 5s - loss: 0.6045 - precision_1: 0.6719 - val_loss: 0.5712 - val_precision_1: 0.7029 - 5s/epoch - 4ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 5s - loss: 0.6024 - precision_1: 0.6704 - val_loss: 0.5866 - val_precision_1: 0.6784 - 5s/epoch - 4ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 5s - loss: 0.5994 - precision_1: 0.6769 - val_loss: 0.5742 - val_precision_1: 0.6930 - 5s/epoch - 4ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 5s - loss: 0.5998 - precision_1: 0.6792 - val_loss: 0.5728 - val_precision_1: 0.6867 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "1054/1054 - 5s - loss: 0.6004 - precision_1: 0.6671 - val_loss: 0.5712 - val_precision_1: 0.6991 - 5s/epoch - 4ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 5s - loss: 0.5972 - precision_1: 0.6785 - val_loss: 0.5709 - val_precision_1: 0.6959 - 5s/epoch - 4ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 5s - loss: 0.5979 - precision_1: 0.6831 - val_loss: 0.5721 - val_precision_1: 0.7069 - 5s/epoch - 5ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 5s - loss: 0.5957 - precision_1: 0.6853 - val_loss: 0.5870 - val_precision_1: 0.6869 - 5s/epoch - 4ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5920 - precision_1: 0.6805 - val_loss: 0.5600 - val_precision_1: 0.7046 - 5s/epoch - 5ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5950 - precision_1: 0.6863 - val_loss: 0.5643 - val_precision_1: 0.7002 - 5s/epoch - 4ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5937 - precision_1: 0.6818 - val_loss: 0.5649 - val_precision_1: 0.7045 - 5s/epoch - 4ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 5s - loss: 0.5900 - precision_1: 0.6844 - val_loss: 0.5659 - val_precision_1: 0.7127 - 5s/epoch - 4ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 5s - loss: 0.5895 - precision_1: 0.6891 - val_loss: 0.5656 - val_precision_1: 0.6990 - 5s/epoch - 4ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 5s - loss: 0.5884 - precision_1: 0.6829 - val_loss: 0.5643 - val_precision_1: 0.6985 - 5s/epoch - 4ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 5s - loss: 0.5862 - precision_1: 0.6933 - val_loss: 0.5639 - val_precision_1: 0.7120 - 5s/epoch - 4ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 5s - loss: 0.5877 - precision_1: 0.6917 - val_loss: 0.5687 - val_precision_1: 0.6930 - 5s/epoch - 4ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 5s - loss: 0.5847 - precision_1: 0.6881 - val_loss: 0.5624 - val_precision_1: 0.7069 - 5s/epoch - 4ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 5s - loss: 0.5801 - precision_1: 0.6949 - val_loss: 0.5576 - val_precision_1: 0.7184 - 5s/epoch - 4ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 5s - loss: 0.5808 - precision_1: 0.6918 - val_loss: 0.5639 - val_precision_1: 0.6973 - 5s/epoch - 4ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 4s - loss: 0.5794 - precision_1: 0.6975 - val_loss: 0.5503 - val_precision_1: 0.7141 - 4s/epoch - 4ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 5s - loss: 0.5800 - precision_1: 0.6926 - val_loss: 0.5587 - val_precision_1: 0.7267 - 5s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 5s - loss: 0.5769 - precision_1: 0.7002 - val_loss: 0.5513 - val_precision_1: 0.7017 - 5s/epoch - 4ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5513 - precision_1: 0.7017\n",
      "Score for fold 2: loss of 0.5512841939926147; precision_1 of 70.17017006874084%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 7s - loss: 0.6561 - precision_2: 0.6328 - val_loss: 0.6427 - val_precision_2: 0.6675 - 7s/epoch - 6ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 6s - loss: 0.6381 - precision_2: 0.6470 - val_loss: 0.6351 - val_precision_2: 0.6752 - 6s/epoch - 6ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 6s - loss: 0.6337 - precision_2: 0.6562 - val_loss: 0.6220 - val_precision_2: 0.6635 - 6s/epoch - 6ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 6s - loss: 0.6297 - precision_2: 0.6530 - val_loss: 0.6263 - val_precision_2: 0.6559 - 6s/epoch - 6ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 6s - loss: 0.6248 - precision_2: 0.6686 - val_loss: 0.6094 - val_precision_2: 0.6690 - 6s/epoch - 6ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 6s - loss: 0.6224 - precision_2: 0.6648 - val_loss: 0.6115 - val_precision_2: 0.6767 - 6s/epoch - 6ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 6s - loss: 0.6247 - precision_2: 0.6659 - val_loss: 0.6109 - val_precision_2: 0.6341 - 6s/epoch - 6ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 6s - loss: 0.6184 - precision_2: 0.6595 - val_loss: 0.6045 - val_precision_2: 0.6461 - 6s/epoch - 6ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 6s - loss: 0.6161 - precision_2: 0.6640 - val_loss: 0.6010 - val_precision_2: 0.6561 - 6s/epoch - 6ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 6s - loss: 0.6156 - precision_2: 0.6626 - val_loss: 0.5996 - val_precision_2: 0.6681 - 6s/epoch - 6ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 6s - loss: 0.6107 - precision_2: 0.6753 - val_loss: 0.6047 - val_precision_2: 0.6638 - 6s/epoch - 6ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 6s - loss: 0.6122 - precision_2: 0.6651 - val_loss: 0.5966 - val_precision_2: 0.6887 - 6s/epoch - 6ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 6s - loss: 0.6057 - precision_2: 0.6799 - val_loss: 0.5941 - val_precision_2: 0.6606 - 6s/epoch - 6ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 6s - loss: 0.6056 - precision_2: 0.6725 - val_loss: 0.5905 - val_precision_2: 0.6684 - 6s/epoch - 6ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 6s - loss: 0.6031 - precision_2: 0.6788 - val_loss: 0.5888 - val_precision_2: 0.6815 - 6s/epoch - 6ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 6s - loss: 0.6003 - precision_2: 0.6847 - val_loss: 0.5908 - val_precision_2: 0.6611 - 6s/epoch - 6ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 6s - loss: 0.5976 - precision_2: 0.6810 - val_loss: 0.5881 - val_precision_2: 0.6849 - 6s/epoch - 6ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 6s - loss: 0.5993 - precision_2: 0.6792 - val_loss: 0.5849 - val_precision_2: 0.6767 - 6s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 6s - loss: 0.5971 - precision_2: 0.6807 - val_loss: 0.5952 - val_precision_2: 0.6796 - 6s/epoch - 6ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 6s - loss: 0.5936 - precision_2: 0.6822 - val_loss: 0.5844 - val_precision_2: 0.6886 - 6s/epoch - 6ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5925 - precision_2: 0.6803 - val_loss: 0.5855 - val_precision_2: 0.6749 - 6s/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 6s - loss: 0.5933 - precision_2: 0.6804 - val_loss: 0.5796 - val_precision_2: 0.6850 - 6s/epoch - 6ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 6s - loss: 0.5895 - precision_2: 0.6902 - val_loss: 0.5792 - val_precision_2: 0.6723 - 6s/epoch - 6ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 6s - loss: 0.5908 - precision_2: 0.6845 - val_loss: 0.5820 - val_precision_2: 0.6873 - 6s/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 6s - loss: 0.5872 - precision_2: 0.6918 - val_loss: 0.5752 - val_precision_2: 0.6786 - 6s/epoch - 6ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 6s - loss: 0.5835 - precision_2: 0.6901 - val_loss: 0.5753 - val_precision_2: 0.6745 - 6s/epoch - 6ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 6s - loss: 0.5837 - precision_2: 0.6961 - val_loss: 0.5917 - val_precision_2: 0.6957 - 6s/epoch - 6ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 6s - loss: 0.5788 - precision_2: 0.6881 - val_loss: 0.5754 - val_precision_2: 0.6589 - 6s/epoch - 6ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 6s - loss: 0.5835 - precision_2: 0.6884 - val_loss: 0.5855 - val_precision_2: 0.6957 - 6s/epoch - 6ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 6s - loss: 0.5784 - precision_2: 0.7003 - val_loss: 0.5773 - val_precision_2: 0.6777 - 6s/epoch - 6ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 6s - loss: 0.5765 - precision_2: 0.6994 - val_loss: 0.5781 - val_precision_2: 0.6850 - 6s/epoch - 6ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 6s - loss: 0.5764 - precision_2: 0.6948 - val_loss: 0.5709 - val_precision_2: 0.6823 - 6s/epoch - 6ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5768 - precision_2: 0.6913 - val_loss: 0.5654 - val_precision_2: 0.6918 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 7s - loss: 0.5742 - precision_2: 0.6992 - val_loss: 0.5678 - val_precision_2: 0.6747 - 7s/epoch - 7ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 7s - loss: 0.5724 - precision_2: 0.6976 - val_loss: 0.5616 - val_precision_2: 0.6845 - 7s/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 7s - loss: 0.5736 - precision_2: 0.7032 - val_loss: 0.5639 - val_precision_2: 0.6824 - 7s/epoch - 6ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 7s - loss: 0.5690 - precision_2: 0.6976 - val_loss: 0.5587 - val_precision_2: 0.6810 - 7s/epoch - 6ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 6s - loss: 0.5651 - precision_2: 0.6982 - val_loss: 0.5572 - val_precision_2: 0.6960 - 6s/epoch - 6ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 6s - loss: 0.5678 - precision_2: 0.6986 - val_loss: 0.5548 - val_precision_2: 0.6951 - 6s/epoch - 6ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 6s - loss: 0.5661 - precision_2: 0.7033 - val_loss: 0.5584 - val_precision_2: 0.6842 - 6s/epoch - 6ms/step\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5584 - precision_2: 0.6842\n",
      "Score for fold 3: loss of 0.5584372282028198; precision_2 of 68.42105388641357%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 7s - loss: 0.6573 - precision_3: 0.6347 - val_loss: 0.6386 - val_precision_3: 0.6767 - 7s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1054/1054 - 6s - loss: 0.6381 - precision_3: 0.6601 - val_loss: 0.6202 - val_precision_3: 0.6865 - 6s/epoch - 6ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 6s - loss: 0.6280 - precision_3: 0.6616 - val_loss: 0.6143 - val_precision_3: 0.6932 - 6s/epoch - 6ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 6s - loss: 0.6257 - precision_3: 0.6721 - val_loss: 0.6159 - val_precision_3: 0.6620 - 6s/epoch - 6ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 7s - loss: 0.6250 - precision_3: 0.6699 - val_loss: 0.6142 - val_precision_3: 0.6745 - 7s/epoch - 6ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 6s - loss: 0.6189 - precision_3: 0.6687 - val_loss: 0.6119 - val_precision_3: 0.6494 - 6s/epoch - 6ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 6s - loss: 0.6175 - precision_3: 0.6759 - val_loss: 0.6147 - val_precision_3: 0.6543 - 6s/epoch - 6ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 6s - loss: 0.6172 - precision_3: 0.6839 - val_loss: 0.6126 - val_precision_3: 0.6469 - 6s/epoch - 6ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 6s - loss: 0.6116 - precision_3: 0.6731 - val_loss: 0.6105 - val_precision_3: 0.6460 - 6s/epoch - 6ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 6s - loss: 0.6129 - precision_3: 0.6785 - val_loss: 0.6112 - val_precision_3: 0.6820 - 6s/epoch - 6ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 6s - loss: 0.6091 - precision_3: 0.6776 - val_loss: 0.6020 - val_precision_3: 0.6806 - 6s/epoch - 6ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 6s - loss: 0.6088 - precision_3: 0.6864 - val_loss: 0.6096 - val_precision_3: 0.6437 - 6s/epoch - 6ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 6s - loss: 0.6094 - precision_3: 0.6775 - val_loss: 0.6037 - val_precision_3: 0.6696 - 6s/epoch - 5ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 6s - loss: 0.6038 - precision_3: 0.6870 - val_loss: 0.6016 - val_precision_3: 0.6777 - 6s/epoch - 5ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 6s - loss: 0.6017 - precision_3: 0.6819 - val_loss: 0.6086 - val_precision_3: 0.6670 - 6s/epoch - 6ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 6s - loss: 0.6019 - precision_3: 0.6852 - val_loss: 0.6005 - val_precision_3: 0.6824 - 6s/epoch - 5ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 6s - loss: 0.5982 - precision_3: 0.6885 - val_loss: 0.6036 - val_precision_3: 0.6888 - 6s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 6s - loss: 0.5982 - precision_3: 0.6888 - val_loss: 0.5935 - val_precision_3: 0.6741 - 6s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 7s - loss: 0.5959 - precision_3: 0.6900 - val_loss: 0.5966 - val_precision_3: 0.6736 - 7s/epoch - 7ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 8s - loss: 0.5971 - precision_3: 0.6852 - val_loss: 0.5944 - val_precision_3: 0.6873 - 8s/epoch - 7ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5914 - precision_3: 0.6926 - val_loss: 0.5941 - val_precision_3: 0.6871 - 6s/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 6s - loss: 0.5923 - precision_3: 0.6926 - val_loss: 0.5892 - val_precision_3: 0.6770 - 6s/epoch - 6ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 6s - loss: 0.5930 - precision_3: 0.6871 - val_loss: 0.5971 - val_precision_3: 0.6878 - 6s/epoch - 5ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 6s - loss: 0.5923 - precision_3: 0.6893 - val_loss: 0.5874 - val_precision_3: 0.6785 - 6s/epoch - 5ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 6s - loss: 0.5876 - precision_3: 0.6995 - val_loss: 0.5922 - val_precision_3: 0.6699 - 6s/epoch - 6ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 6s - loss: 0.5887 - precision_3: 0.6918 - val_loss: 0.5919 - val_precision_3: 0.6895 - 6s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5870 - precision_3: 0.7019 - val_loss: 0.5942 - val_precision_3: 0.6860 - 5s/epoch - 5ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5861 - precision_3: 0.6940 - val_loss: 0.5966 - val_precision_3: 0.6819 - 5s/epoch - 5ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5848 - precision_3: 0.7012 - val_loss: 0.5871 - val_precision_3: 0.6896 - 5s/epoch - 5ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 6s - loss: 0.5821 - precision_3: 0.6982 - val_loss: 0.5923 - val_precision_3: 0.6986 - 6s/epoch - 5ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 6s - loss: 0.5829 - precision_3: 0.7002 - val_loss: 0.5878 - val_precision_3: 0.6954 - 6s/epoch - 6ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 6s - loss: 0.5804 - precision_3: 0.7017 - val_loss: 0.5840 - val_precision_3: 0.6934 - 6s/epoch - 5ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5799 - precision_3: 0.7001 - val_loss: 0.5837 - val_precision_3: 0.6729 - 6s/epoch - 5ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5767 - precision_3: 0.7015 - val_loss: 0.5863 - val_precision_3: 0.6747 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 6s - loss: 0.5756 - precision_3: 0.7038 - val_loss: 0.5798 - val_precision_3: 0.6795 - 6s/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 5s - loss: 0.5754 - precision_3: 0.7062 - val_loss: 0.5887 - val_precision_3: 0.7062 - 5s/epoch - 5ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 5s - loss: 0.5725 - precision_3: 0.7063 - val_loss: 0.5784 - val_precision_3: 0.6965 - 5s/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 6s - loss: 0.5735 - precision_3: 0.6998 - val_loss: 0.5814 - val_precision_3: 0.6791 - 6s/epoch - 5ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 6s - loss: 0.5706 - precision_3: 0.7042 - val_loss: 0.5825 - val_precision_3: 0.6690 - 6s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 5s - loss: 0.5700 - precision_3: 0.7074 - val_loss: 0.5713 - val_precision_3: 0.7088 - 5s/epoch - 5ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5713 - precision_3: 0.7088\n",
      "Score for fold 4: loss of 0.5712677836418152; precision_3 of 70.88167071342468%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 6s - loss: 0.6582 - precision_4: 0.6296 - val_loss: 0.6193 - val_precision_4: 0.6674 - 6s/epoch - 6ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 5s - loss: 0.6352 - precision_4: 0.6593 - val_loss: 0.6164 - val_precision_4: 0.6917 - 5s/epoch - 5ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 5s - loss: 0.6314 - precision_4: 0.6541 - val_loss: 0.6070 - val_precision_4: 0.6918 - 5s/epoch - 5ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 5s - loss: 0.6300 - precision_4: 0.6561 - val_loss: 0.6084 - val_precision_4: 0.6856 - 5s/epoch - 5ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 5s - loss: 0.6217 - precision_4: 0.6631 - val_loss: 0.6084 - val_precision_4: 0.6960 - 5s/epoch - 5ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 5s - loss: 0.6202 - precision_4: 0.6615 - val_loss: 0.5978 - val_precision_4: 0.7049 - 5s/epoch - 5ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 5s - loss: 0.6180 - precision_4: 0.6648 - val_loss: 0.5969 - val_precision_4: 0.6667 - 5s/epoch - 5ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 5s - loss: 0.6176 - precision_4: 0.6639 - val_loss: 0.5978 - val_precision_4: 0.6742 - 5s/epoch - 5ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 5s - loss: 0.6155 - precision_4: 0.6651 - val_loss: 0.5999 - val_precision_4: 0.6493 - 5s/epoch - 5ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 5s - loss: 0.6155 - precision_4: 0.6651 - val_loss: 0.6026 - val_precision_4: 0.6618 - 5s/epoch - 5ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 5s - loss: 0.6130 - precision_4: 0.6685 - val_loss: 0.5963 - val_precision_4: 0.6647 - 5s/epoch - 5ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 5s - loss: 0.6088 - precision_4: 0.6678 - val_loss: 0.5910 - val_precision_4: 0.6990 - 5s/epoch - 5ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 5s - loss: 0.6074 - precision_4: 0.6696 - val_loss: 0.5859 - val_precision_4: 0.6947 - 5s/epoch - 5ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 5s - loss: 0.6041 - precision_4: 0.6753 - val_loss: 0.5869 - val_precision_4: 0.6798 - 5s/epoch - 5ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 5s - loss: 0.6017 - precision_4: 0.6664 - val_loss: 0.5946 - val_precision_4: 0.6641 - 5s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 5s - loss: 0.6016 - precision_4: 0.6764 - val_loss: 0.5823 - val_precision_4: 0.6822 - 5s/epoch - 5ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 5s - loss: 0.5987 - precision_4: 0.6846 - val_loss: 0.5885 - val_precision_4: 0.6781 - 5s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 6s - loss: 0.5989 - precision_4: 0.6831 - val_loss: 0.5797 - val_precision_4: 0.6958 - 6s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 6s - loss: 0.5943 - precision_4: 0.6787 - val_loss: 0.5767 - val_precision_4: 0.6840 - 6s/epoch - 6ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 6s - loss: 0.5950 - precision_4: 0.6784 - val_loss: 0.5838 - val_precision_4: 0.6924 - 6s/epoch - 6ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5901 - precision_4: 0.6841 - val_loss: 0.5778 - val_precision_4: 0.7016 - 6s/epoch - 5ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 6s - loss: 0.5889 - precision_4: 0.6854 - val_loss: 0.5811 - val_precision_4: 0.6948 - 6s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "1054/1054 - 5s - loss: 0.5878 - precision_4: 0.6914 - val_loss: 0.5833 - val_precision_4: 0.6784 - 5s/epoch - 5ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 5s - loss: 0.5868 - precision_4: 0.6905 - val_loss: 0.5758 - val_precision_4: 0.6914 - 5s/epoch - 5ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 5s - loss: 0.5881 - precision_4: 0.6877 - val_loss: 0.5740 - val_precision_4: 0.6818 - 5s/epoch - 5ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 5s - loss: 0.5835 - precision_4: 0.6922 - val_loss: 0.5665 - val_precision_4: 0.7074 - 5s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5810 - precision_4: 0.6911 - val_loss: 0.5709 - val_precision_4: 0.7003 - 5s/epoch - 5ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5815 - precision_4: 0.6910 - val_loss: 0.5634 - val_precision_4: 0.7008 - 5s/epoch - 5ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5810 - precision_4: 0.6953 - val_loss: 0.5633 - val_precision_4: 0.7030 - 5s/epoch - 5ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 5s - loss: 0.5792 - precision_4: 0.6983 - val_loss: 0.5667 - val_precision_4: 0.6862 - 5s/epoch - 5ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 6s - loss: 0.5771 - precision_4: 0.7010 - val_loss: 0.5600 - val_precision_4: 0.6932 - 6s/epoch - 5ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 6s - loss: 0.5740 - precision_4: 0.7038 - val_loss: 0.5611 - val_precision_4: 0.7191 - 6s/epoch - 6ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5751 - precision_4: 0.7067 - val_loss: 0.5689 - val_precision_4: 0.6962 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5721 - precision_4: 0.7091 - val_loss: 0.5587 - val_precision_4: 0.6922 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 6s - loss: 0.5703 - precision_4: 0.7128 - val_loss: 0.5587 - val_precision_4: 0.7045 - 6s/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 6s - loss: 0.5685 - precision_4: 0.7105 - val_loss: 0.5529 - val_precision_4: 0.7194 - 6s/epoch - 6ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 6s - loss: 0.5647 - precision_4: 0.7146 - val_loss: 0.5538 - val_precision_4: 0.7168 - 6s/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 5s - loss: 0.5613 - precision_4: 0.7119 - val_loss: 0.5566 - val_precision_4: 0.7088 - 5s/epoch - 5ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 5s - loss: 0.5634 - precision_4: 0.7114 - val_loss: 0.5573 - val_precision_4: 0.6833 - 5s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 5s - loss: 0.5601 - precision_4: 0.7157 - val_loss: 0.5577 - val_precision_4: 0.6919 - 5s/epoch - 5ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5577 - precision_4: 0.6919\n",
      "Score for fold 5: loss of 0.5576803088188171; precision_4 of 69.19339299201965%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 7s - loss: 0.6615 - precision_5: 0.6159 - val_loss: 0.6267 - val_precision_5: 0.6400 - 7s/epoch - 6ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 6s - loss: 0.6419 - precision_5: 0.6429 - val_loss: 0.6246 - val_precision_5: 0.6472 - 6s/epoch - 6ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 6s - loss: 0.6325 - precision_5: 0.6535 - val_loss: 0.6117 - val_precision_5: 0.6562 - 6s/epoch - 5ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 5s - loss: 0.6281 - precision_5: 0.6658 - val_loss: 0.6027 - val_precision_5: 0.6886 - 5s/epoch - 5ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 5s - loss: 0.6244 - precision_5: 0.6738 - val_loss: 0.6063 - val_precision_5: 0.6851 - 5s/epoch - 5ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 5s - loss: 0.6221 - precision_5: 0.6644 - val_loss: 0.5975 - val_precision_5: 0.6607 - 5s/epoch - 5ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 6s - loss: 0.6207 - precision_5: 0.6610 - val_loss: 0.6013 - val_precision_5: 0.6816 - 6s/epoch - 5ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 5s - loss: 0.6169 - precision_5: 0.6620 - val_loss: 0.5924 - val_precision_5: 0.6753 - 5s/epoch - 5ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 5s - loss: 0.6143 - precision_5: 0.6635 - val_loss: 0.5977 - val_precision_5: 0.6925 - 5s/epoch - 5ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 5s - loss: 0.6138 - precision_5: 0.6717 - val_loss: 0.6003 - val_precision_5: 0.6649 - 5s/epoch - 5ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 5s - loss: 0.6124 - precision_5: 0.6668 - val_loss: 0.5846 - val_precision_5: 0.6854 - 5s/epoch - 5ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 5s - loss: 0.6086 - precision_5: 0.6709 - val_loss: 0.5804 - val_precision_5: 0.6727 - 5s/epoch - 5ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 5s - loss: 0.6076 - precision_5: 0.6724 - val_loss: 0.5879 - val_precision_5: 0.6680 - 5s/epoch - 5ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 5s - loss: 0.6016 - precision_5: 0.6788 - val_loss: 0.5899 - val_precision_5: 0.6691 - 5s/epoch - 5ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 5s - loss: 0.6043 - precision_5: 0.6746 - val_loss: 0.5816 - val_precision_5: 0.7010 - 5s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 5s - loss: 0.6016 - precision_5: 0.6749 - val_loss: 0.5740 - val_precision_5: 0.6892 - 5s/epoch - 5ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 5s - loss: 0.6009 - precision_5: 0.6765 - val_loss: 0.5761 - val_precision_5: 0.6914 - 5s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 5s - loss: 0.5984 - precision_5: 0.6769 - val_loss: 0.5721 - val_precision_5: 0.7003 - 5s/epoch - 5ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 5s - loss: 0.5991 - precision_5: 0.6874 - val_loss: 0.5715 - val_precision_5: 0.7007 - 5s/epoch - 5ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 5s - loss: 0.5916 - precision_5: 0.6855 - val_loss: 0.5742 - val_precision_5: 0.7159 - 5s/epoch - 5ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 5s - loss: 0.5955 - precision_5: 0.6898 - val_loss: 0.5641 - val_precision_5: 0.7111 - 5s/epoch - 5ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 5s - loss: 0.5916 - precision_5: 0.6927 - val_loss: 0.5620 - val_precision_5: 0.6829 - 5s/epoch - 5ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 5s - loss: 0.5886 - precision_5: 0.6898 - val_loss: 0.5667 - val_precision_5: 0.7218 - 5s/epoch - 5ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 5s - loss: 0.5911 - precision_5: 0.6909 - val_loss: 0.5628 - val_precision_5: 0.6995 - 5s/epoch - 5ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 5s - loss: 0.5855 - precision_5: 0.6950 - val_loss: 0.5656 - val_precision_5: 0.7086 - 5s/epoch - 4ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 5s - loss: 0.5872 - precision_5: 0.6901 - val_loss: 0.5548 - val_precision_5: 0.7019 - 5s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5804 - precision_5: 0.6929 - val_loss: 0.5554 - val_precision_5: 0.6985 - 5s/epoch - 4ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5817 - precision_5: 0.6901 - val_loss: 0.5538 - val_precision_5: 0.7037 - 5s/epoch - 5ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5813 - precision_5: 0.6910 - val_loss: 0.5671 - val_precision_5: 0.6980 - 5s/epoch - 5ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 5s - loss: 0.5802 - precision_5: 0.6999 - val_loss: 0.5470 - val_precision_5: 0.6951 - 5s/epoch - 5ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 5s - loss: 0.5759 - precision_5: 0.6873 - val_loss: 0.5583 - val_precision_5: 0.7221 - 5s/epoch - 5ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 5s - loss: 0.5752 - precision_5: 0.6942 - val_loss: 0.5437 - val_precision_5: 0.7186 - 5s/epoch - 5ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 5s - loss: 0.5739 - precision_5: 0.6987 - val_loss: 0.5525 - val_precision_5: 0.7206 - 5s/epoch - 5ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 5s - loss: 0.5683 - precision_5: 0.7010 - val_loss: 0.5447 - val_precision_5: 0.7191 - 5s/epoch - 5ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 5s - loss: 0.5699 - precision_5: 0.6951 - val_loss: 0.5391 - val_precision_5: 0.7085 - 5s/epoch - 5ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 5s - loss: 0.5708 - precision_5: 0.6985 - val_loss: 0.5348 - val_precision_5: 0.7220 - 5s/epoch - 5ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 5s - loss: 0.5689 - precision_5: 0.7030 - val_loss: 0.5424 - val_precision_5: 0.7331 - 5s/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 5s - loss: 0.5688 - precision_5: 0.7028 - val_loss: 0.5373 - val_precision_5: 0.7210 - 5s/epoch - 5ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 5s - loss: 0.5661 - precision_5: 0.7007 - val_loss: 0.5320 - val_precision_5: 0.7055 - 5s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 5s - loss: 0.5633 - precision_5: 0.7033 - val_loss: 0.5366 - val_precision_5: 0.7234 - 5s/epoch - 5ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5366 - precision_5: 0.7234\n",
      "Score for fold 6: loss of 0.5366407632827759; precision_5 of 72.34042286872864%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 6s - loss: 0.6537 - precision_6: 0.6363 - val_loss: 0.6146 - val_precision_6: 0.6694 - 6s/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1054/1054 - 5s - loss: 0.6401 - precision_6: 0.6587 - val_loss: 0.6244 - val_precision_6: 0.6367 - 5s/epoch - 5ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 5s - loss: 0.6271 - precision_6: 0.6640 - val_loss: 0.6124 - val_precision_6: 0.6766 - 5s/epoch - 5ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 5s - loss: 0.6271 - precision_6: 0.6740 - val_loss: 0.6272 - val_precision_6: 0.6927 - 5s/epoch - 5ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 5s - loss: 0.6226 - precision_6: 0.6716 - val_loss: 0.6021 - val_precision_6: 0.6583 - 5s/epoch - 5ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 5s - loss: 0.6203 - precision_6: 0.6671 - val_loss: 0.6127 - val_precision_6: 0.6708 - 5s/epoch - 5ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 5s - loss: 0.6184 - precision_6: 0.6650 - val_loss: 0.6012 - val_precision_6: 0.6510 - 5s/epoch - 4ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 5s - loss: 0.6174 - precision_6: 0.6720 - val_loss: 0.6063 - val_precision_6: 0.6733 - 5s/epoch - 4ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 5s - loss: 0.6135 - precision_6: 0.6788 - val_loss: 0.6043 - val_precision_6: 0.6696 - 5s/epoch - 5ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 5s - loss: 0.6137 - precision_6: 0.6750 - val_loss: 0.5949 - val_precision_6: 0.6659 - 5s/epoch - 4ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 5s - loss: 0.6107 - precision_6: 0.6794 - val_loss: 0.6066 - val_precision_6: 0.6451 - 5s/epoch - 5ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 5s - loss: 0.6088 - precision_6: 0.6702 - val_loss: 0.5973 - val_precision_6: 0.6656 - 5s/epoch - 4ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 5s - loss: 0.6070 - precision_6: 0.6781 - val_loss: 0.5925 - val_precision_6: 0.6599 - 5s/epoch - 4ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 5s - loss: 0.6047 - precision_6: 0.6711 - val_loss: 0.5905 - val_precision_6: 0.6594 - 5s/epoch - 4ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 5s - loss: 0.6039 - precision_6: 0.6719 - val_loss: 0.5950 - val_precision_6: 0.6729 - 5s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 5s - loss: 0.6051 - precision_6: 0.6796 - val_loss: 0.5902 - val_precision_6: 0.6588 - 5s/epoch - 4ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 5s - loss: 0.6006 - precision_6: 0.6787 - val_loss: 0.5881 - val_precision_6: 0.6660 - 5s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 6s - loss: 0.5973 - precision_6: 0.6830 - val_loss: 0.5840 - val_precision_6: 0.6673 - 6s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 6s - loss: 0.5975 - precision_6: 0.6763 - val_loss: 0.5848 - val_precision_6: 0.6620 - 6s/epoch - 5ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 6s - loss: 0.5959 - precision_6: 0.6822 - val_loss: 0.5865 - val_precision_6: 0.6630 - 6s/epoch - 6ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5931 - precision_6: 0.6826 - val_loss: 0.5847 - val_precision_6: 0.6826 - 6s/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 5s - loss: 0.5915 - precision_6: 0.6870 - val_loss: 0.5817 - val_precision_6: 0.6809 - 5s/epoch - 5ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 6s - loss: 0.5928 - precision_6: 0.6794 - val_loss: 0.5809 - val_precision_6: 0.6852 - 6s/epoch - 6ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 6s - loss: 0.5915 - precision_6: 0.6865 - val_loss: 0.5837 - val_precision_6: 0.6708 - 6s/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 5s - loss: 0.5862 - precision_6: 0.6867 - val_loss: 0.5773 - val_precision_6: 0.6770 - 5s/epoch - 5ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 5s - loss: 0.5876 - precision_6: 0.6854 - val_loss: 0.5737 - val_precision_6: 0.6768 - 5s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 5s - loss: 0.5885 - precision_6: 0.6884 - val_loss: 0.5707 - val_precision_6: 0.6787 - 5s/epoch - 5ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 5s - loss: 0.5855 - precision_6: 0.6959 - val_loss: 0.5773 - val_precision_6: 0.6693 - 5s/epoch - 4ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 5s - loss: 0.5842 - precision_6: 0.6924 - val_loss: 0.5845 - val_precision_6: 0.6887 - 5s/epoch - 5ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 5s - loss: 0.5824 - precision_6: 0.6903 - val_loss: 0.5795 - val_precision_6: 0.6638 - 5s/epoch - 5ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 6s - loss: 0.5843 - precision_6: 0.6886 - val_loss: 0.5739 - val_precision_6: 0.6927 - 6s/epoch - 5ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 5s - loss: 0.5828 - precision_6: 0.6929 - val_loss: 0.5739 - val_precision_6: 0.6794 - 5s/epoch - 5ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5820 - precision_6: 0.6982 - val_loss: 0.5717 - val_precision_6: 0.6811 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5785 - precision_6: 0.6891 - val_loss: 0.5731 - val_precision_6: 0.6782 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 6s - loss: 0.5766 - precision_6: 0.6936 - val_loss: 0.5669 - val_precision_6: 0.6706 - 6s/epoch - 5ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 6s - loss: 0.5748 - precision_6: 0.6955 - val_loss: 0.5683 - val_precision_6: 0.6926 - 6s/epoch - 5ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 5s - loss: 0.5728 - precision_6: 0.7035 - val_loss: 0.5704 - val_precision_6: 0.6927 - 5s/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 6s - loss: 0.5737 - precision_6: 0.7020 - val_loss: 0.5736 - val_precision_6: 0.6644 - 6s/epoch - 6ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 6s - loss: 0.5732 - precision_6: 0.7013 - val_loss: 0.5646 - val_precision_6: 0.6981 - 6s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 6s - loss: 0.5728 - precision_6: 0.6959 - val_loss: 0.5653 - val_precision_6: 0.6908 - 6s/epoch - 5ms/step\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5653 - precision_6: 0.6908\n",
      "Score for fold 7: loss of 0.5653278827667236; precision_6 of 69.07545328140259%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 7s - loss: 0.6577 - precision_7: 0.6302 - val_loss: 0.6129 - val_precision_7: 0.6730 - 7s/epoch - 7ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 8s - loss: 0.6332 - precision_7: 0.6542 - val_loss: 0.6141 - val_precision_7: 0.7250 - 8s/epoch - 8ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 9s - loss: 0.6300 - precision_7: 0.6597 - val_loss: 0.6066 - val_precision_7: 0.6567 - 9s/epoch - 9ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 9s - loss: 0.6291 - precision_7: 0.6637 - val_loss: 0.6142 - val_precision_7: 0.6844 - 9s/epoch - 8ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 7s - loss: 0.6237 - precision_7: 0.6604 - val_loss: 0.5968 - val_precision_7: 0.6646 - 7s/epoch - 7ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 7s - loss: 0.6226 - precision_7: 0.6679 - val_loss: 0.6033 - val_precision_7: 0.6395 - 7s/epoch - 7ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 8s - loss: 0.6231 - precision_7: 0.6621 - val_loss: 0.5971 - val_precision_7: 0.6910 - 8s/epoch - 8ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 8s - loss: 0.6204 - precision_7: 0.6676 - val_loss: 0.5998 - val_precision_7: 0.6935 - 8s/epoch - 7ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 9s - loss: 0.6175 - precision_7: 0.6776 - val_loss: 0.6020 - val_precision_7: 0.6854 - 9s/epoch - 8ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 7s - loss: 0.6123 - precision_7: 0.6736 - val_loss: 0.5945 - val_precision_7: 0.6773 - 7s/epoch - 7ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 7s - loss: 0.6157 - precision_7: 0.6743 - val_loss: 0.6011 - val_precision_7: 0.6950 - 7s/epoch - 7ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 9s - loss: 0.6115 - precision_7: 0.6816 - val_loss: 0.5888 - val_precision_7: 0.6777 - 9s/epoch - 8ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 8s - loss: 0.6103 - precision_7: 0.6785 - val_loss: 0.5954 - val_precision_7: 0.6695 - 8s/epoch - 8ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 8s - loss: 0.6071 - precision_7: 0.6819 - val_loss: 0.5881 - val_precision_7: 0.6724 - 8s/epoch - 8ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 8s - loss: 0.6078 - precision_7: 0.6896 - val_loss: 0.5895 - val_precision_7: 0.6772 - 8s/epoch - 8ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 7s - loss: 0.6048 - precision_7: 0.6802 - val_loss: 0.5770 - val_precision_7: 0.6794 - 7s/epoch - 7ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 7s - loss: 0.5999 - precision_7: 0.6758 - val_loss: 0.5790 - val_precision_7: 0.6998 - 7s/epoch - 6ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 7s - loss: 0.5989 - precision_7: 0.6862 - val_loss: 0.5784 - val_precision_7: 0.6872 - 7s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 7s - loss: 0.5990 - precision_7: 0.6774 - val_loss: 0.5802 - val_precision_7: 0.6908 - 7s/epoch - 7ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 6s - loss: 0.5976 - precision_7: 0.6882 - val_loss: 0.5713 - val_precision_7: 0.6901 - 6s/epoch - 6ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5963 - precision_7: 0.6788 - val_loss: 0.5760 - val_precision_7: 0.6899 - 6s/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 7s - loss: 0.5937 - precision_7: 0.6855 - val_loss: 0.5829 - val_precision_7: 0.6750 - 7s/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "1054/1054 - 6s - loss: 0.5948 - precision_7: 0.6902 - val_loss: 0.5734 - val_precision_7: 0.6869 - 6s/epoch - 6ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 6s - loss: 0.5944 - precision_7: 0.6866 - val_loss: 0.5662 - val_precision_7: 0.7088 - 6s/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 6s - loss: 0.5906 - precision_7: 0.6955 - val_loss: 0.5688 - val_precision_7: 0.6958 - 6s/epoch - 6ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 7s - loss: 0.5861 - precision_7: 0.6980 - val_loss: 0.5641 - val_precision_7: 0.7017 - 7s/epoch - 6ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 6s - loss: 0.5886 - precision_7: 0.6928 - val_loss: 0.5597 - val_precision_7: 0.7070 - 6s/epoch - 6ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 7s - loss: 0.5838 - precision_7: 0.6995 - val_loss: 0.5630 - val_precision_7: 0.7043 - 7s/epoch - 6ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 7s - loss: 0.5846 - precision_7: 0.6940 - val_loss: 0.5655 - val_precision_7: 0.6918 - 7s/epoch - 7ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 7s - loss: 0.5819 - precision_7: 0.6988 - val_loss: 0.5591 - val_precision_7: 0.6952 - 7s/epoch - 6ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 7s - loss: 0.5790 - precision_7: 0.6985 - val_loss: 0.5549 - val_precision_7: 0.7063 - 7s/epoch - 6ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 6s - loss: 0.5787 - precision_7: 0.6979 - val_loss: 0.5603 - val_precision_7: 0.7041 - 6s/epoch - 6ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5791 - precision_7: 0.6988 - val_loss: 0.5531 - val_precision_7: 0.6971 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5754 - precision_7: 0.6991 - val_loss: 0.5519 - val_precision_7: 0.6966 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 7s - loss: 0.5759 - precision_7: 0.6949 - val_loss: 0.5507 - val_precision_7: 0.7110 - 7s/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 6s - loss: 0.5721 - precision_7: 0.7018 - val_loss: 0.5572 - val_precision_7: 0.7152 - 6s/epoch - 6ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 6s - loss: 0.5711 - precision_7: 0.6991 - val_loss: 0.5722 - val_precision_7: 0.6734 - 6s/epoch - 6ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 6s - loss: 0.5712 - precision_7: 0.6966 - val_loss: 0.5576 - val_precision_7: 0.6986 - 6s/epoch - 6ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 7s - loss: 0.5693 - precision_7: 0.7003 - val_loss: 0.5499 - val_precision_7: 0.7146 - 7s/epoch - 6ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 7s - loss: 0.5673 - precision_7: 0.7089 - val_loss: 0.5574 - val_precision_7: 0.7089 - 7s/epoch - 6ms/step\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.5574 - precision_7: 0.7089\n",
      "Score for fold 8: loss of 0.5574355125427246; precision_7 of 70.88607549667358%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 9s - loss: 0.6580 - precision_8: 0.6354 - val_loss: 0.6352 - val_precision_8: 0.6792 - 9s/epoch - 8ms/step\n",
      "Epoch 2/40\n",
      "1054/1054 - 6s - loss: 0.6428 - precision_8: 0.6632 - val_loss: 0.6196 - val_precision_8: 0.6791 - 6s/epoch - 6ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 7s - loss: 0.6293 - precision_8: 0.6759 - val_loss: 0.6152 - val_precision_8: 0.6659 - 7s/epoch - 6ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 6s - loss: 0.6278 - precision_8: 0.6776 - val_loss: 0.6148 - val_precision_8: 0.6819 - 6s/epoch - 6ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 8s - loss: 0.6290 - precision_8: 0.6876 - val_loss: 0.6171 - val_precision_8: 0.6789 - 8s/epoch - 7ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 7s - loss: 0.6261 - precision_8: 0.6736 - val_loss: 0.6204 - val_precision_8: 0.6707 - 7s/epoch - 6ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 7s - loss: 0.6239 - precision_8: 0.6773 - val_loss: 0.6161 - val_precision_8: 0.6784 - 7s/epoch - 6ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 7s - loss: 0.6207 - precision_8: 0.6804 - val_loss: 0.6200 - val_precision_8: 0.6639 - 7s/epoch - 7ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 7s - loss: 0.6212 - precision_8: 0.6688 - val_loss: 0.6120 - val_precision_8: 0.6716 - 7s/epoch - 7ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 7s - loss: 0.6171 - precision_8: 0.6718 - val_loss: 0.6190 - val_precision_8: 0.7050 - 7s/epoch - 7ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 7s - loss: 0.6155 - precision_8: 0.6783 - val_loss: 0.6054 - val_precision_8: 0.7039 - 7s/epoch - 7ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 7s - loss: 0.6151 - precision_8: 0.6860 - val_loss: 0.5991 - val_precision_8: 0.6935 - 7s/epoch - 7ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 8s - loss: 0.6140 - precision_8: 0.6791 - val_loss: 0.6054 - val_precision_8: 0.6663 - 8s/epoch - 7ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 7s - loss: 0.6105 - precision_8: 0.6748 - val_loss: 0.5995 - val_precision_8: 0.7025 - 7s/epoch - 7ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 6s - loss: 0.6083 - precision_8: 0.6782 - val_loss: 0.6019 - val_precision_8: 0.6894 - 6s/epoch - 6ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 8s - loss: 0.6070 - precision_8: 0.6778 - val_loss: 0.5985 - val_precision_8: 0.6642 - 8s/epoch - 7ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 9s - loss: 0.6092 - precision_8: 0.6830 - val_loss: 0.5982 - val_precision_8: 0.6780 - 9s/epoch - 8ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 8s - loss: 0.6059 - precision_8: 0.6777 - val_loss: 0.5910 - val_precision_8: 0.6844 - 8s/epoch - 7ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 7s - loss: 0.6034 - precision_8: 0.6743 - val_loss: 0.5982 - val_precision_8: 0.6730 - 7s/epoch - 7ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 7s - loss: 0.6044 - precision_8: 0.6822 - val_loss: 0.6017 - val_precision_8: 0.6701 - 7s/epoch - 7ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 7s - loss: 0.6011 - precision_8: 0.6831 - val_loss: 0.5950 - val_precision_8: 0.6811 - 7s/epoch - 7ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 7s - loss: 0.6020 - precision_8: 0.6919 - val_loss: 0.5900 - val_precision_8: 0.6835 - 7s/epoch - 6ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 7s - loss: 0.5965 - precision_8: 0.6837 - val_loss: 0.5935 - val_precision_8: 0.6937 - 7s/epoch - 6ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 7s - loss: 0.5965 - precision_8: 0.6881 - val_loss: 0.5895 - val_precision_8: 0.6880 - 7s/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 7s - loss: 0.5961 - precision_8: 0.6888 - val_loss: 0.5909 - val_precision_8: 0.6882 - 7s/epoch - 6ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 6s - loss: 0.5949 - precision_8: 0.6871 - val_loss: 0.5796 - val_precision_8: 0.7073 - 6s/epoch - 6ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 7s - loss: 0.5951 - precision_8: 0.6895 - val_loss: 0.5835 - val_precision_8: 0.6870 - 7s/epoch - 6ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 6s - loss: 0.5924 - precision_8: 0.6917 - val_loss: 0.5858 - val_precision_8: 0.6731 - 6s/epoch - 6ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 7s - loss: 0.5895 - precision_8: 0.6918 - val_loss: 0.5825 - val_precision_8: 0.6851 - 7s/epoch - 6ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 7s - loss: 0.5919 - precision_8: 0.6915 - val_loss: 0.5817 - val_precision_8: 0.6878 - 7s/epoch - 7ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 7s - loss: 0.5889 - precision_8: 0.6898 - val_loss: 0.5827 - val_precision_8: 0.7042 - 7s/epoch - 7ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 7s - loss: 0.5906 - precision_8: 0.6934 - val_loss: 0.5711 - val_precision_8: 0.6853 - 7s/epoch - 7ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5872 - precision_8: 0.6908 - val_loss: 0.5694 - val_precision_8: 0.6950 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5843 - precision_8: 0.6942 - val_loss: 0.5739 - val_precision_8: 0.6942 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 7s - loss: 0.5844 - precision_8: 0.7054 - val_loss: 0.5710 - val_precision_8: 0.7098 - 7s/epoch - 7ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 9s - loss: 0.5847 - precision_8: 0.7001 - val_loss: 0.5686 - val_precision_8: 0.7038 - 9s/epoch - 8ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 7s - loss: 0.5828 - precision_8: 0.7015 - val_loss: 0.5692 - val_precision_8: 0.6999 - 7s/epoch - 7ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 7s - loss: 0.5777 - precision_8: 0.7060 - val_loss: 0.5728 - val_precision_8: 0.6865 - 7s/epoch - 6ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 7s - loss: 0.5815 - precision_8: 0.7028 - val_loss: 0.5663 - val_precision_8: 0.7047 - 7s/epoch - 6ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 7s - loss: 0.5819 - precision_8: 0.7025 - val_loss: 0.5717 - val_precision_8: 0.7012 - 7s/epoch - 7ms/step\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5717 - precision_8: 0.7012\n",
      "Score for fold 9: loss of 0.5716983079910278; precision_8 of 70.11865973472595%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/40\n",
      "1054/1054 - 8s - loss: 0.6610 - precision_9: 0.6200 - val_loss: 0.6330 - val_precision_9: 0.6868 - 8s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1054/1054 - 6s - loss: 0.6428 - precision_9: 0.6563 - val_loss: 0.6193 - val_precision_9: 0.6587 - 6s/epoch - 6ms/step\n",
      "Epoch 3/40\n",
      "1054/1054 - 6s - loss: 0.6339 - precision_9: 0.6702 - val_loss: 0.6195 - val_precision_9: 0.6827 - 6s/epoch - 6ms/step\n",
      "Epoch 4/40\n",
      "1054/1054 - 6s - loss: 0.6272 - precision_9: 0.6725 - val_loss: 0.6148 - val_precision_9: 0.6799 - 6s/epoch - 6ms/step\n",
      "Epoch 5/40\n",
      "1054/1054 - 6s - loss: 0.6269 - precision_9: 0.6694 - val_loss: 0.6192 - val_precision_9: 0.6692 - 6s/epoch - 6ms/step\n",
      "Epoch 6/40\n",
      "1054/1054 - 7s - loss: 0.6245 - precision_9: 0.6743 - val_loss: 0.6070 - val_precision_9: 0.6593 - 7s/epoch - 6ms/step\n",
      "Epoch 7/40\n",
      "1054/1054 - 6s - loss: 0.6220 - precision_9: 0.6767 - val_loss: 0.6132 - val_precision_9: 0.6898 - 6s/epoch - 6ms/step\n",
      "Epoch 8/40\n",
      "1054/1054 - 6s - loss: 0.6204 - precision_9: 0.6825 - val_loss: 0.6107 - val_precision_9: 0.6730 - 6s/epoch - 6ms/step\n",
      "Epoch 9/40\n",
      "1054/1054 - 6s - loss: 0.6170 - precision_9: 0.6757 - val_loss: 0.6152 - val_precision_9: 0.7266 - 6s/epoch - 6ms/step\n",
      "Epoch 10/40\n",
      "1054/1054 - 6s - loss: 0.6174 - precision_9: 0.6776 - val_loss: 0.6066 - val_precision_9: 0.6795 - 6s/epoch - 6ms/step\n",
      "Epoch 11/40\n",
      "1054/1054 - 6s - loss: 0.6138 - precision_9: 0.6763 - val_loss: 0.6013 - val_precision_9: 0.6725 - 6s/epoch - 6ms/step\n",
      "Epoch 12/40\n",
      "1054/1054 - 6s - loss: 0.6126 - precision_9: 0.6809 - val_loss: 0.6103 - val_precision_9: 0.6839 - 6s/epoch - 6ms/step\n",
      "Epoch 13/40\n",
      "1054/1054 - 6s - loss: 0.6096 - precision_9: 0.6843 - val_loss: 0.6059 - val_precision_9: 0.6674 - 6s/epoch - 6ms/step\n",
      "Epoch 14/40\n",
      "1054/1054 - 6s - loss: 0.6082 - precision_9: 0.6799 - val_loss: 0.5950 - val_precision_9: 0.6809 - 6s/epoch - 6ms/step\n",
      "Epoch 15/40\n",
      "1054/1054 - 6s - loss: 0.6086 - precision_9: 0.6831 - val_loss: 0.6002 - val_precision_9: 0.6928 - 6s/epoch - 6ms/step\n",
      "Epoch 16/40\n",
      "1054/1054 - 6s - loss: 0.6055 - precision_9: 0.6870 - val_loss: 0.5975 - val_precision_9: 0.6895 - 6s/epoch - 6ms/step\n",
      "Epoch 17/40\n",
      "1054/1054 - 6s - loss: 0.6023 - precision_9: 0.6859 - val_loss: 0.5933 - val_precision_9: 0.6639 - 6s/epoch - 6ms/step\n",
      "Epoch 18/40\n",
      "1054/1054 - 6s - loss: 0.6015 - precision_9: 0.6840 - val_loss: 0.5869 - val_precision_9: 0.6866 - 6s/epoch - 6ms/step\n",
      "Epoch 19/40\n",
      "1054/1054 - 6s - loss: 0.6010 - precision_9: 0.6854 - val_loss: 0.5909 - val_precision_9: 0.6844 - 6s/epoch - 6ms/step\n",
      "Epoch 20/40\n",
      "1054/1054 - 6s - loss: 0.5984 - precision_9: 0.6860 - val_loss: 0.5896 - val_precision_9: 0.6770 - 6s/epoch - 6ms/step\n",
      "Epoch 21/40\n",
      "1054/1054 - 6s - loss: 0.5995 - precision_9: 0.6849 - val_loss: 0.5860 - val_precision_9: 0.6824 - 6s/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "1054/1054 - 6s - loss: 0.5975 - precision_9: 0.6868 - val_loss: 0.5899 - val_precision_9: 0.7093 - 6s/epoch - 6ms/step\n",
      "Epoch 23/40\n",
      "1054/1054 - 6s - loss: 0.5929 - precision_9: 0.6959 - val_loss: 0.5897 - val_precision_9: 0.6646 - 6s/epoch - 6ms/step\n",
      "Epoch 24/40\n",
      "1054/1054 - 6s - loss: 0.5957 - precision_9: 0.6838 - val_loss: 0.5889 - val_precision_9: 0.6892 - 6s/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "1054/1054 - 6s - loss: 0.5960 - precision_9: 0.6878 - val_loss: 0.5854 - val_precision_9: 0.6967 - 6s/epoch - 6ms/step\n",
      "Epoch 26/40\n",
      "1054/1054 - 6s - loss: 0.5905 - precision_9: 0.6985 - val_loss: 0.5842 - val_precision_9: 0.6884 - 6s/epoch - 6ms/step\n",
      "Epoch 27/40\n",
      "1054/1054 - 6s - loss: 0.5925 - precision_9: 0.6899 - val_loss: 0.5803 - val_precision_9: 0.7023 - 6s/epoch - 6ms/step\n",
      "Epoch 28/40\n",
      "1054/1054 - 6s - loss: 0.5885 - precision_9: 0.6946 - val_loss: 0.5736 - val_precision_9: 0.6842 - 6s/epoch - 6ms/step\n",
      "Epoch 29/40\n",
      "1054/1054 - 6s - loss: 0.5881 - precision_9: 0.6955 - val_loss: 0.5826 - val_precision_9: 0.6811 - 6s/epoch - 6ms/step\n",
      "Epoch 30/40\n",
      "1054/1054 - 6s - loss: 0.5873 - precision_9: 0.6949 - val_loss: 0.5769 - val_precision_9: 0.6976 - 6s/epoch - 6ms/step\n",
      "Epoch 31/40\n",
      "1054/1054 - 7s - loss: 0.5840 - precision_9: 0.6915 - val_loss: 0.5747 - val_precision_9: 0.7024 - 7s/epoch - 6ms/step\n",
      "Epoch 32/40\n",
      "1054/1054 - 6s - loss: 0.5864 - precision_9: 0.6931 - val_loss: 0.5717 - val_precision_9: 0.7204 - 6s/epoch - 6ms/step\n",
      "Epoch 33/40\n",
      "1054/1054 - 6s - loss: 0.5800 - precision_9: 0.7048 - val_loss: 0.5710 - val_precision_9: 0.7100 - 6s/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "1054/1054 - 6s - loss: 0.5827 - precision_9: 0.6999 - val_loss: 0.5702 - val_precision_9: 0.7123 - 6s/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "1054/1054 - 6s - loss: 0.5826 - precision_9: 0.7038 - val_loss: 0.5690 - val_precision_9: 0.6993 - 6s/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "1054/1054 - 6s - loss: 0.5808 - precision_9: 0.6986 - val_loss: 0.5600 - val_precision_9: 0.7172 - 6s/epoch - 6ms/step\n",
      "Epoch 37/40\n",
      "1054/1054 - 6s - loss: 0.5761 - precision_9: 0.7074 - val_loss: 0.5750 - val_precision_9: 0.7013 - 6s/epoch - 6ms/step\n",
      "Epoch 38/40\n",
      "1054/1054 - 6s - loss: 0.5747 - precision_9: 0.6947 - val_loss: 0.5651 - val_precision_9: 0.7116 - 6s/epoch - 6ms/step\n",
      "Epoch 39/40\n",
      "1054/1054 - 6s - loss: 0.5718 - precision_9: 0.7019 - val_loss: 0.5696 - val_precision_9: 0.6964 - 6s/epoch - 6ms/step\n",
      "Epoch 40/40\n",
      "1054/1054 - 6s - loss: 0.5729 - precision_9: 0.7030 - val_loss: 0.5615 - val_precision_9: 0.7130 - 6s/epoch - 6ms/step\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5615 - precision_9: 0.7130\n",
      "Score for fold 10: loss of 0.5615198016166687; precision_9 of 71.30144834518433%\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "\n",
    "for train, test in skf.split(X, y):   \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 1, activation='relu',input_shape=(16, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Conv1D(64, 3, activation='relu', padding='same'))    \n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    "    model.add(Conv1D(128, 1, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 3, activation='relu', padding='same'))    \n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    "    model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 1, activation='relu',input_shape=(21, 1)))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    "    model.add(Conv1D(128, 1, activation='relu', padding='same'))\n",
    "    #model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "   # model.add(Dropout(.5))\n",
    "    model.add(MaxPooling1D(pool_size = 2, padding='same'))\n",
    "   # model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu', input_shape=(21, 1)))\n",
    "    #model.add(Dropout(.2))   \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Conv2D(128, (1, 1), activation='relu', padding='same'))\n",
    "    #model.add(Dropout(.2))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))   \n",
    "    \n",
    "    \"\"\"\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Precision()])\n",
    "  #  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    history = model.fit(X[train], y[train], batch_size = 15, epochs = 40, validation_data=(X[test], y[test]), verbose = 2)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X[test], y[test], verbose = 1)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "  #  acc_train_his.append(history.history['accuracy'])\n",
    " #   acc_val_his.append(history.history['val_accuracy'])\n",
    "    \n",
    "    loss_train_his.append(history.history['loss'])\n",
    "    loss_val_his.append(history.history['val_loss'])\n",
    "    \n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1678721143918,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "N02Q8-ilbE8O",
    "outputId": "6ac2c0d1-9ed0-4bf6-eb06-52d939604823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 79.2212975025177\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Acc: {np.mean(np.array(acc_per_fold))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "executionInfo": {
     "elapsed": 1847,
     "status": "ok",
     "timestamp": 1678721164170,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "kcl9fp2KbE1e",
    "outputId": "ea36ad49-60d5-4a55-b3b0-0fd6b76b4bce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(25, 10))\n",
    "\n",
    "for i in range(len(acc_train_his)):\n",
    "    axs[0 + i // 5, i % 5].plot(loss_train_his[i])\n",
    "    axs[0 + i // 5, i % 5].plot(loss_val_his[i])\n",
    "    axs[0 + i // 5, i % 5].set_title(f'model loss fold{i + 1}')\n",
    "    axs[0 + i // 5, i % 5].legend([f'train{i}', f'val{i}'], loc='upper left')\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='epoch', ylabel='accuracy')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1678721174255,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "8UdQDtwUd7Id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from scipy.io.arff import loadarff \n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import chi2, RFE\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, StackingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score, average_precision_score, \\\n",
    "    mean_squared_error, r2_score, brier_score_loss\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import pearsonr\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import logging\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15970,
     "status": "ok",
     "timestamp": 1678721197009,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "8QiOHrzybEg4",
    "outputId": "9f00adb3-0b75-42fd-fbc4-5baa5949f2ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: 0.9746543778801844\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 2: 0.967741935483871\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 3: 0.9792147806004619\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 4: 0.9676674364896074\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 5: 0.9491916859122402\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 6: 0.9584295612009238\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 7: 0.9491916859122402\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 8: 0.9676674364896074\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 9: 0.9653579676674365\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 10: 0.9722863741339491\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import y1\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "pre_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []      \n",
    "for train, test in skf.split(X, y):\n",
    "    \n",
    "    # CLASSIFICATION\n",
    "    # Random Forest\n",
    "    model = RandomForestClassifier( criterion='gini', min_samples_split=2,min_samples_leaf=1,  min_weight_fraction_leaf=0.0,max_features='auto', max_leaf_nodes=None,  random_state=0, verbose=0, warm_start=False, class_weight='balanced'  )\n",
    "    # Decision Tree\n",
    "    \n",
    " #   model = DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=2, min_samples_leaf=1,random_state=0, class_weight='balanced')\n",
    "    \n",
    "    # Logistic Regression\n",
    " #   model = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "        \n",
    "    # SVM\n",
    "  #  model = SVC(random_state=0, class_weight='balanced', kernel='linear')\n",
    "    \n",
    "    # Extra trees\n",
    "   # model = ExtraTreesClassifier(random_state=0, class_weight='balanced', criterion='gini')\n",
    "        \n",
    "    # Adaboost\n",
    " #   model = AdaBoostClassifier(random_state=0, n_estimators=600)\n",
    " \n",
    "    # XGBoost\n",
    " #   model = xgb.XGBClassifier(random_state=0)\n",
    "      #MLP\n",
    "#    model = MLPClassifier(random_state=0)\n",
    "\n",
    "   # model = HistGradientBoostingClassifier(random_state=0)\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    history = model.fit(X[train], y[train])\n",
    "    pred_y = model.predict(X[test])\n",
    "    report = classification_report(y[test], pred_y, output_dict=True)\n",
    "    \"\"\"\n",
    "    disp = plot_confusion_matrix(model, X[test], y[test],\n",
    "                                display_labels=[0, 1],\n",
    "                                cmap=plt.cm.Blues)\n",
    "                                \"\"\"\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    score = report['accuracy']\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "    weighted_precision = report['weighted avg']['precision']\n",
    "    weighted_recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    auc = roc_auc_score(y[test], model.predict_proba(X[test])[:, 1])\n",
    "    \"\"\"\n",
    "    tp = disp.confusion_matrix[1][1]\n",
    "    fp = disp.confusion_matrix[0][1]\n",
    "    fn = disp.confusion_matrix[1][0]\n",
    "    tn = disp.confusion_matrix[0][0]\n",
    "    \"\"\"\n",
    "    \n",
    "#     results['kc2'][i+1][\"vanilla_gan\"]['randomforest'] = [score, precision, recall, f1, auc, tp, fp, tn, fn, weighted_precision, weighted_recall, weighted_f1]\n",
    "    \n",
    "    scores = model.score(X[test], y[test])\n",
    "    print(f'Score for fold {fold_no}: {scores}')\n",
    "    acc_per_fold.append(scores)\n",
    "    pre_per_fold.append(precision)\n",
    "    recall_per_fold.append(recall)\n",
    "    f1_per_fold.append(f1)\n",
    "    fold_no = fold_no + 1\n",
    "    #auc = auc+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1678721277967,
     "user": {
      "displayName": "Hà Thị Minh Phương",
      "userId": "03221265603374510364"
     },
     "user_tz": 0
    },
    "id": "oezjkuy2akPi",
    "outputId": "78786a62-8215-41a4-e3e1-91fc85d0dd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 0.9651403241770522\n",
      "Mean Acc: 0.9640120864662801\n",
      "Mean Acc: 0.967975942805128\n",
      "Mean Acc: 0.9649629903281214\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Acc: {np.mean(np.array(acc_per_fold))}')\n",
    "print(f'Mean Acc: {np.mean(np.array(pre_per_fold))}')\n",
    "print(f'Mean Acc: {np.mean(np.array(recall_per_fold))}')\n",
    "print(f'Mean Acc: {np.mean(np.array(f1_per_fold))}')\n",
    "#print(f'Mean Acc: {np.mean(np.array(auc))}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
